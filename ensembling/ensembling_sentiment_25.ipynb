{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16dc321f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (4.9.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (4.61.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: requests in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (1.20.2)\n",
      "Requirement already satisfied: sacremoses in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: packaging in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from huggingface-hub==0.0.12->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: click in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: six in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9be0096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  4 18:15:15 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     On   | 00000000:3D:00.0 Off |                  Off |\n",
      "| 33%   31C    P8    35W / 260W |      3MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     On   | 00000000:3F:00.0 Off |                  Off |\n",
      "| 33%   33C    P8    23W / 260W |      3MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro RTX 8000     On   | 00000000:40:00.0 Off |                  Off |\n",
      "| 33%   33C    P8    23W / 260W |      3MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 8000     On   | 00000000:41:00.0 Off |                  Off |\n",
      "| 33%   30C    P8    32W / 260W |      3MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81d3aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#change sheetname\n",
    "# train_df=pd.read_excel('../../data/cotraining1.xlsx', sheet_name=0)\n",
    "# test_df=pd.read_excel('../../data/data.xlsx', sheet_name=2)\n",
    "val_df=pd.read_excel('../../data/data.xlsx', sheet_name=1)\n",
    "test_df=pd.read_excel('../../data/connor.xlsx', sheet_name=0)\n",
    "# noisy_train_df=pd.read_excel('../../data/noisy_train_cot_0.xlsx', sheet_name=0)\n",
    "bad_chars = [';', ':', \"(\", \")\", \"-\", \"'\", \"\\\"\", \"_\", \".\", \",\", \" \"]\n",
    "\n",
    "def components(df):\n",
    "    statements = list(df['statements'])\n",
    "    labels = list(df['labels'])\n",
    "    genes = list(df['target_genes'])\n",
    "    return statements, labels, genes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c3b273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target_genes</th>\n",
       "      <th>statements</th>\n",
       "      <th>labels</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>OXA-51</td>\n",
       "      <td>Both are ambler class D ss-lactamases, which o...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ttyh</td>\n",
       "      <td>Tweety-homolog (Ttyh) Family Encodes the Pore-...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MXI1</td>\n",
       "      <td>Regardless of the oxygen, in senescent cells P...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>cst</td>\n",
       "      <td>OBJECTIVES: Stress and burn-out among surgical...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>opu</td>\n",
       "      <td>Social support (via Facebook) may have a posit...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17797</th>\n",
       "      <td>17797</td>\n",
       "      <td>pqe</td>\n",
       "      <td>Target pre-cueing conditions did not affect th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17798</th>\n",
       "      <td>17798</td>\n",
       "      <td>MexA</td>\n",
       "      <td>Two P . aeruginosa efflux pumps, MexAB-OprM an...</td>\n",
       "      <td>1</td>\n",
       "      <td>7109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17799</th>\n",
       "      <td>17799</td>\n",
       "      <td>mfd</td>\n",
       "      <td>6c) . The most abundant ARG, mfd, conferring r...</td>\n",
       "      <td>1</td>\n",
       "      <td>9253.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17800</th>\n",
       "      <td>17800</td>\n",
       "      <td>hch</td>\n",
       "      <td>Fokin, and Irina M . Roshchevskaya . Body surf...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17801</th>\n",
       "      <td>17801</td>\n",
       "      <td>AQU-1</td>\n",
       "      <td>In another study 13 of 16 of A . dhakensis blo...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17802 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 target_genes  \\\n",
       "0               0       OXA-51   \n",
       "1               1         ttyh   \n",
       "2               2         MXI1   \n",
       "3               3          cst   \n",
       "4               4          opu   \n",
       "...           ...          ...   \n",
       "17797       17797          pqe   \n",
       "17798       17798         MexA   \n",
       "17799       17799          mfd   \n",
       "17800       17800          hch   \n",
       "17801       17801        AQU-1   \n",
       "\n",
       "                                              statements  labels  Unnamed: 0.1  \n",
       "0      Both are ambler class D ss-lactamases, which o...       1           NaN  \n",
       "1      Tweety-homolog (Ttyh) Family Encodes the Pore-...       0           NaN  \n",
       "2      Regardless of the oxygen, in senescent cells P...       0           NaN  \n",
       "3      OBJECTIVES: Stress and burn-out among surgical...       0           NaN  \n",
       "4      Social support (via Facebook) may have a posit...       0           NaN  \n",
       "...                                                  ...     ...           ...  \n",
       "17797  Target pre-cueing conditions did not affect th...       0           NaN  \n",
       "17798  Two P . aeruginosa efflux pumps, MexAB-OprM an...       1        7109.0  \n",
       "17799  6c) . The most abundant ARG, mfd, conferring r...       1        9253.0  \n",
       "17800  Fokin, and Irina M . Roshchevskaya . Body surf...       0           NaN  \n",
       "17801  In another study 13 of 16 of A . dhakensis blo...       1           NaN  \n",
       "\n",
       "[17802 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6442b4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target_genes</th>\n",
       "      <th>statements</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AAC(2')-Ia</td>\n",
       "      <td>The enzymes generally promote the acetylation ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AAC(2')-Ia</td>\n",
       "      <td>Many bacterial pathogens acetylate their pepti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AAC(2')-Ia</td>\n",
       "      <td>In Providencia stuartii, a Gram-negative speci...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AAC(2')-Ia</td>\n",
       "      <td>smegmatis, and characterization of AG resistan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AAC(2')-Ia</td>\n",
       "      <td>In leaderless mRNAs, there is no Shine-Dalgarn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18470</th>\n",
       "      <td>18470</td>\n",
       "      <td>YojI</td>\n",
       "      <td>We demonstrate that YojI is capable of pumping...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18471</th>\n",
       "      <td>18471</td>\n",
       "      <td>YojI</td>\n",
       "      <td>The outer membrane protein TolC in addition to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18472</th>\n",
       "      <td>18472</td>\n",
       "      <td>YojI</td>\n",
       "      <td>Thus, one obvious explanation for the protecti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18473</th>\n",
       "      <td>18473</td>\n",
       "      <td>YojI</td>\n",
       "      <td>In the present study, we showed that yojI, an ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18474</th>\n",
       "      <td>18474</td>\n",
       "      <td>YojI</td>\n",
       "      <td>In the present study, we showed that yojI, an ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18475 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 target_genes  \\\n",
       "0               0   AAC(2')-Ia   \n",
       "1               1   AAC(2')-Ia   \n",
       "2               2   AAC(2')-Ia   \n",
       "3               3   AAC(2')-Ia   \n",
       "4               4   AAC(2')-Ia   \n",
       "...           ...          ...   \n",
       "18470       18470         YojI   \n",
       "18471       18471         YojI   \n",
       "18472       18472         YojI   \n",
       "18473       18473         YojI   \n",
       "18474       18474         YojI   \n",
       "\n",
       "                                              statements  labels  \n",
       "0      The enzymes generally promote the acetylation ...       1  \n",
       "1      Many bacterial pathogens acetylate their pepti...       1  \n",
       "2      In Providencia stuartii, a Gram-negative speci...       1  \n",
       "3      smegmatis, and characterization of AG resistan...       1  \n",
       "4      In leaderless mRNAs, there is no Shine-Dalgarn...       1  \n",
       "...                                                  ...     ...  \n",
       "18470  We demonstrate that YojI is capable of pumping...       1  \n",
       "18471  The outer membrane protein TolC in addition to...       1  \n",
       "18472  Thus, one obvious explanation for the protecti...       1  \n",
       "18473  In the present study, we showed that yojI, an ...       1  \n",
       "18474  In the present study, we showed that yojI, an ...       1  \n",
       "\n",
       "[18475 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noisy_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e28a0035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target_genes</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>statements</th>\n",
       "      <th>review_label</th>\n",
       "      <th>label</th>\n",
       "      <th>Remarks</th>\n",
       "      <th>labels</th>\n",
       "      <th>connor_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1436</td>\n",
       "      <td>msbA</td>\n",
       "      <td>4979069</td>\n",
       "      <td>coli conclude that the conformational transiti...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>ABC transporter</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>343</td>\n",
       "      <td>CpxR</td>\n",
       "      <td>5063474</td>\n",
       "      <td>The results showed that, apart from the previo...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1382</td>\n",
       "      <td>mphI</td>\n",
       "      <td>5760710</td>\n",
       "      <td>MphI shares high sequence identity (94%) to ho...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Note that the actual conclusion of the linked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>187</td>\n",
       "      <td>cpxA</td>\n",
       "      <td>3319533</td>\n",
       "      <td>The disruption at cpxAR operon was confirmed w...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>plasmid</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1854</td>\n",
       "      <td>sdiA</td>\n",
       "      <td>2812512</td>\n",
       "      <td>It was recently discovered that S . Typhimuriu...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>874</td>\n",
       "      <td>kamB</td>\n",
       "      <td>2760815</td>\n",
       "      <td>tenebrarius, for which a modification to the o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1481</td>\n",
       "      <td>msbA</td>\n",
       "      <td>4979069</td>\n",
       "      <td>Fluorescence was followed in a LS 55B Luminesc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>613</td>\n",
       "      <td>FosB</td>\n",
       "      <td>3985756</td>\n",
       "      <td>FosBSa crystallized with Zn2+ in the active si...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>not enough information in this snippet to know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>102</td>\n",
       "      <td>ANT(2'')-Ia</td>\n",
       "      <td>4313920</td>\n",
       "      <td>3A) . Comparative structural analysis searches...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>569</td>\n",
       "      <td>FosB</td>\n",
       "      <td>3985756</td>\n",
       "      <td>The analysis of the data was completed accordi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0 target_genes    pmcid  \\\n",
       "0         1436         msbA  4979069   \n",
       "1          343         CpxR  5063474   \n",
       "2         1382         mphI  5760710   \n",
       "3          187         cpxA  3319533   \n",
       "4         1854         sdiA  2812512   \n",
       "..         ...          ...      ...   \n",
       "95         874         kamB  2760815   \n",
       "96        1481         msbA  4979069   \n",
       "97         613         FosB  3985756   \n",
       "98         102  ANT(2'')-Ia  4313920   \n",
       "99         569         FosB  3985756   \n",
       "\n",
       "                                           statements  review_label  label  \\\n",
       "0   coli conclude that the conformational transiti...          -1.0      1   \n",
       "1   The results showed that, apart from the previo...          -1.0     -1   \n",
       "2   MphI shares high sequence identity (94%) to ho...           1.0      1   \n",
       "3   The disruption at cpxAR operon was confirmed w...          -1.0      1   \n",
       "4   It was recently discovered that S . Typhimuriu...          -1.0     -1   \n",
       "..                                                ...           ...    ...   \n",
       "95  tenebrarius, for which a modification to the o...           NaN      1   \n",
       "96  Fluorescence was followed in a LS 55B Luminesc...           NaN      1   \n",
       "97  FosBSa crystallized with Zn2+ in the active si...           NaN     -1   \n",
       "98  3A) . Comparative structural analysis searches...           NaN      1   \n",
       "99  The analysis of the data was completed accordi...           NaN      1   \n",
       "\n",
       "             Remarks  labels  \\\n",
       "0   ABC transporter        0   \n",
       "1                NaN       0   \n",
       "2                NaN       0   \n",
       "3            plasmid       0   \n",
       "4                NaN       0   \n",
       "..               ...     ...   \n",
       "95               NaN       0   \n",
       "96               NaN       0   \n",
       "97               NaN       0   \n",
       "98               NaN       1   \n",
       "99               NaN       0   \n",
       "\n",
       "                                         connor_notes  \n",
       "0                                                 NaN  \n",
       "1                                                 NaN  \n",
       "2   Note that the actual conclusion of the linked ...  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "..                                                ...  \n",
       "95                                                NaN  \n",
       "96                                                NaN  \n",
       "97     not enough information in this snippet to know  \n",
       "98                                                NaN  \n",
       "99                                                NaN  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0d4153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_genes</th>\n",
       "      <th>statements</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDM-1</td>\n",
       "      <td>5% in Indian and Pakistan hospitals . In addit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>otrC</td>\n",
       "      <td>Additionally, the significantly enhanced vanco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FosB</td>\n",
       "      <td>Thus, the P1 space group appears to be the res...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CpxR</td>\n",
       "      <td>In Klebsiella pneumoniae, CpxR is involved in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mfd</td>\n",
       "      <td>3) . Thus, the reduced spontaneous mutation ra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>rphB</td>\n",
       "      <td>225-25 muM rifampin and 68 . 5 nM RphB . Enzym...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>emrE</td>\n",
       "      <td>Thus, EmrE can exist in solution in a stable n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>sdiA</td>\n",
       "      <td>Additionally, a second microarray study was pe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>CTX-M-94</td>\n",
       "      <td>3 and 7 . 5, for both CTX-M-94 and -100 enzyme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>FosB</td>\n",
       "      <td>Hydrogen bonding of Tyr39 to Arg124 (see Figur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    target_genes                                         statements  labels\n",
       "0          NDM-1  5% in Indian and Pakistan hospitals . In addit...       1\n",
       "1           otrC  Additionally, the significantly enhanced vanco...       1\n",
       "2           FosB  Thus, the P1 space group appears to be the res...       1\n",
       "3           CpxR  In Klebsiella pneumoniae, CpxR is involved in ...       1\n",
       "4            mfd  3) . Thus, the reduced spontaneous mutation ra...       1\n",
       "..           ...                                                ...     ...\n",
       "281         rphB  225-25 muM rifampin and 68 . 5 nM RphB . Enzym...       0\n",
       "282         emrE  Thus, EmrE can exist in solution in a stable n...       0\n",
       "283         sdiA  Additionally, a second microarray study was pe...       0\n",
       "284     CTX-M-94  3 and 7 . 5, for both CTX-M-94 and -100 enzyme...       0\n",
       "285         FosB  Hydrogen bonding of Tyr39 to Arg124 (see Figur...       0\n",
       "\n",
       "[286 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14fb25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_statements, train_labels, genes=components(train_df)\n",
    "test_statements, test_labels, test_genes=components(test_df)\n",
    "val_statements, val_labels, val_genes=components(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000ae9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_train_statements, noisy_train_labels, noisy_train_genes=components(noisy_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da3d7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "Device name: Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15575ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f771105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    \n",
    "set_seed(42)    # Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0677c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b2ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import BertForSequenceClassification\n",
    "# model = BertForSequenceClassification.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe6ad4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# # Encode our concatenated data\n",
    "# encoded_seq = [tokenizer.encode(seq, add_special_tokens=True) for seq in statements]\n",
    "\n",
    "# token_len=[]\n",
    "# # Find the maximum length\n",
    "# token_len=[len(sent) for sent in encoded_seq]\n",
    "# # print('Max length: ', max_len)\n",
    "# sns.distplot(token_len)\n",
    "# plt.xlim([0, 1024]);\n",
    "# plt.xlabel('Token count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ec7ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#position_embeddings\n",
    "\n",
    "def pids(statements, genes):\n",
    "    positional_embeddings=[]\n",
    "    for idx in range(len(statements)):\n",
    "        tokens=statements[idx].split()\n",
    "        pid_list=[0]\n",
    "        for token in tokens:\n",
    "            sub_tokens=tokenizer.tokenize(token) #{'prepare_for_tokenization':True})\n",
    "            word=tokenizer.convert_tokens_to_string(sub_tokens)\n",
    "            filtered_gene=''.join(i for i in genes[idx] if not i in bad_chars)\n",
    "            filtered_word=''.join(i for i in word if not i in bad_chars)\n",
    "            if filtered_gene.lower() in filtered_word.lower():\n",
    "                temp_list=[1]*len(sub_tokens) \n",
    "            else:\n",
    "                temp_list=[0]*len(sub_tokens)\n",
    "            pid_list = [y for x in [pid_list, temp_list] for y in x]\n",
    "        if(len(pid_list)>512):\n",
    "            del pid_list[512:]\n",
    "        elif (len(pid_list)==512 and pid_list[511]==1):\n",
    "            pid_list[511]=0\n",
    "        else:\n",
    "            temp_list=[0]*(512-len(pid_list))\n",
    "            pid_list = [y for x in [pid_list, temp_list] for y in x]\n",
    "        # print(pid_list)\n",
    "        positional_embeddings.append(pid_list)\n",
    "        # pretokenized_string=tokenizer.convert_tokens_to_string(res_list)\n",
    "        # print(pretokenized_string)\n",
    "#     large_df['positional_embeddings']=positional_embeddings\n",
    "    return positional_embeddings    \n",
    "\n",
    "\n",
    "# positional_embeddings=small_df['positional_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8be06486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare mask keywords\n",
    "\n",
    "bad_chars = [';', ':', \"(\", \")\", \"-\", \"'\", \"\\\"\", \"_\", \".\", \",\", \" \"]\n",
    "\n",
    "rule1=['resistan', 'efflux']\n",
    "\n",
    "rule2=[\"beta-lactamase\", \"beta lactamase\",  \"streptogramin inactivation enzyme\", \n",
    "\"fosfomycin inactivation enzyme\",\n",
    "\"tetracycline inactivation enzyme \", #AMR gene\n",
    "\"macrolide inactivation enzyme\",\n",
    "\"rifampin inactivation enzyme\",\n",
    "\"aminoglycoside acetyltransferase\", #\"AAC\",\n",
    "\"chloramphenicol phosphotransferase\",\n",
    "\"aminoglycoside phosphotransferase\", #\"APH\",\n",
    "\"chloramphenicol acetyltransferase\", #\"CAT\", \n",
    "\"aminoglycoside nucleotidyltransferase\", #\"ANT\",\n",
    "\"lincosamide nucleotidyltransferase\", #\"LNU\" \n",
    "\"streptothricin acetyltransferase\", #\"SAT\", \n",
    "\"fusidic acid inactivation enzyme\",\n",
    "\"Edeine acetyltransferase\", \n",
    "\"viomycin phosphotransferase\", \n",
    "\"ciprofloxacin phosphotransferase\",\n",
    "\"Bah amidohydrolase\",\n",
    "\"MDR\",\n",
    "\"MRSA\"]\n",
    "\n",
    "rule3=['bla', 'mec']\n",
    "\n",
    "rule4=['MIC', 'increase', 'fold']\n",
    "\n",
    "antibiotics_df=pd.read_excel('../../data/antibiotics.xlsx', sheet_name=0)\n",
    "temp_keywords=list(antibiotics_df['antibiotics'])\n",
    "temp_keywords = [y for x in [temp_keywords, rule1] for y in x]\n",
    "temp_keywords = [y for x in [temp_keywords, rule2] for y in x]\n",
    "temp_keywords = [y for x in [temp_keywords, rule3] for y in x]\n",
    "temp_keywords = [y for x in [temp_keywords, rule4] for y in x]\n",
    "mask_keywords=[]\n",
    "for i in range(len(temp_keywords)):\n",
    "    keywords=temp_keywords[i].replace(\"-\", \" \").split()\n",
    "#     for j in range(len(keywords)):\n",
    "#         filtered_word=''.join(k for k in keywords[j] if not k in bad_chars)\n",
    "    mask_keywords = [y for x in [mask_keywords, keywords] for y in x]\n",
    "mask_keywords=[i for i in mask_keywords if len(i)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4228236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_keywords=random.sample(mask_keywords, int(len(mask_keywords)*0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfd94b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(statements, labels, genes):\n",
    "    ##########################################################  \n",
    "\n",
    "    #sub_tokens doesn't have [SEP] at the end\n",
    "    #Exclude CLS SEP and prompt\n",
    "    all_masks=[]\n",
    "    for idx in range(len(statements)):\n",
    "        tokens=statements[idx].split()\n",
    "        masks=[0]\n",
    "        for token in tokens:\n",
    "            sub_tokens=tokenizer.tokenize(token) #{'prepare_for_tokenization':True})\n",
    "            word=tokenizer.convert_tokens_to_string(sub_tokens)\n",
    "            #bla+target_gene\n",
    "            filtered_gene=''.join(i for i in genes[idx] if not i in bad_chars)\n",
    "            filtered_word=''.join(i for i in word if not i in bad_chars)\n",
    "            if filtered_gene.lower() in filtered_word.lower():\n",
    "                temp_list=[0]*len(sub_tokens)\n",
    "            else:\n",
    "                flag=0\n",
    "                for keyword in mask_keywords:\n",
    "                    filtered_keyword=''.join(i for i in keyword if not i in bad_chars)\n",
    "                    if filtered_keyword.lower() in filtered_word.lower():\n",
    "                        flag=1\n",
    "                        temp_list=[1]*len(sub_tokens)\n",
    "                        break\n",
    "                if flag==0:\n",
    "                    temp_list=[0]*len(sub_tokens)\n",
    "            masks = [y for x in [masks, temp_list] for y in x]\n",
    "        if(len(masks)>=512):\n",
    "            del masks[512:]\n",
    "            masks[511]=0\n",
    "#         elif (len(masks)==512 and masks[511]==1):\n",
    "#             masks[511]=0\n",
    "#         else:\n",
    "#             temp_list=[0]*(512-len(masks))\n",
    "#             masks = [y for x in [masks, temp_list] for y in x]\n",
    "#         # print(masks)\n",
    "        all_masks.append(masks)\n",
    "        \n",
    "    return all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "126336da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_rule_masks=preprocessing_for_bert(train_statements, train_labels, genes)\n",
    "val_rule_masks=preprocessing_for_bert(val_statements, val_labels, val_genes)\n",
    "test_rule_masks=preprocessing_for_bert(test_statements, test_labels, test_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "272034b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_train_rule_masks=preprocessing_for_bert(noisy_train_statements, noisy_train_labels, noisy_train_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0647f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_positional_embeddings=pids(train_statements, genes)\n",
    "val_positional_embeddings=pids(val_statements, val_genes)\n",
    "test_positional_embeddings=pids(test_statements, test_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5ebe687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_train_positional_embeddings=pids(noisy_train_statements, noisy_train_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff7dc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train_statements\n",
    "X_val = val_statements\n",
    "# y_train = train_labels\n",
    "y_val = val_labels\n",
    "\n",
    "# X_train_pids = train_positional_embeddings\n",
    "X_val_pids = val_positional_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d322844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_noisy_train = noisy_train_statements\n",
    "# y_noisy_train = noisy_train_labels\n",
    "# X_noisy_train_pids = noisy_train_positional_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a119fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, labels, pids):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length', \n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs['labels']=torch.tensor(labels)\n",
    "    inputs['pids']=torch.tensor(pids)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ac9efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_MLM(text, labels, pids, rule_masks):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length', \n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs['labels']=torch.tensor(labels)\n",
    "    inputs['pids']=torch.tensor(pids)\n",
    "    inputs['labels_MLM']=torch.ones(len(inputs['input_ids']),512, dtype=int) \n",
    "    #################\n",
    "    ## MLM ##\n",
    "    for idx in range(len(inputs['input_ids'])):\n",
    "        rule_mask=rule_masks[idx]\n",
    "        for i in range(len(rule_mask)):\n",
    "            if rule_mask[i]==1:\n",
    "                inputs['labels_MLM'][idx][i]=inputs['input_ids'][idx][i]\n",
    "                inputs['input_ids'][idx][i]=tokenizer.mask_token_id\n",
    "    #################\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0284c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.encodings['input_ids'][index]\n",
    "        labels = self.encodings['labels'][index]\n",
    "        attention_mask = self.encodings['attention_mask'][index]\n",
    "        token_type_ids = self.encodings['token_type_ids'][index]\n",
    "        pids = self.encodings['pids'][index]\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'position_ids': pids\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "126f6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDataset_MLM(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.encodings['input_ids'][index]\n",
    "        labels = self.encodings['labels'][index]\n",
    "        attention_mask = self.encodings['attention_mask'][index]\n",
    "        token_type_ids = self.encodings['token_type_ids'][index]\n",
    "        pids = self.encodings['pids'][index]\n",
    "        labels_MLM=self.encodings['labels_MLM'][index]\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'position_ids': pids,\n",
    "            'labels_MLM': labels_MLM\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b543254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_inputs=tokenize(X_train, y_train, X_train_pids)\n",
    "val_inputs=tokenize(X_val, y_val, X_val_pids)\n",
    "test_inputs=tokenize(test_statements, test_labels, test_positional_embeddings)\n",
    "\n",
    "# train_dataset = BookDataset(train_inputs)\n",
    "val_dataset = BookDataset(val_inputs)\n",
    "test_dataset = BookDataset(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a6ed7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_train_inputs = tokenize(X_noisy_train, y_noisy_train, X_noisy_train_pids)\n",
    "# noisy_train_dataset = BookDataset(noisy_train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39186d18",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-69840a825904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_inputs_MLM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_MLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_pids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rule_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_dataset_MLM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBookDataset_MLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs_MLM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "train_inputs_MLM=tokenize_MLM(X_train, y_train, X_train_pids, train_rule_masks)\n",
    "train_dataset_MLM = BookDataset_MLM(train_inputs_MLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a38964e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=16,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# val_dataloader = torch.utils.data.DataLoader(\n",
    "#     val_dataset,\n",
    "#     batch_size=16,\n",
    "# )\n",
    "\n",
    "# test_dataloader = torch.utils.data.DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=16,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0de8d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d9b150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader):\n",
    "    set_seed(42)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-05)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "#     for epoch in range(epochs):\n",
    "    #     loop = tqdm(train_dataloader)\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pids = batch['position_ids'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,position_ids=pids, labels=labels)\n",
    "    #     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    #     outputs = model(**batch)\n",
    "        loss = outputs.loss   \n",
    "        train_loss.append(loss.item())\n",
    "        b_probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(b_probabilities, dim=1)\n",
    "    #     print(\"preds: \", preds[0])\n",
    "    #     print(\"labels: \", labels[0])\n",
    "        # Calculate the accuracy rate\n",
    "        for i in range(len(labels)):\n",
    "            if preds[i].item() == 1 and labels[i].item() == 1: \n",
    "                tp+=1\n",
    "            elif preds[i].item() == 1 and labels[i].item() == 0:\n",
    "                fp+=1\n",
    "            elif preds[i].item() == 0 and labels[i].item() == 0:\n",
    "                tn+=1\n",
    "            elif preds[i].item() == 0 and labels[i].item() == 1:\n",
    "                fn+=1\n",
    "        accuracy = (preds == labels).cpu().numpy().mean() * 100\n",
    "        train_accuracy.append(accuracy)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = np.mean(train_loss)\n",
    "    train_accuracy = np.mean(train_accuracy)\n",
    "    \n",
    "    beta=0.5\n",
    "    train_precision_pos=tp/(tp+fp)\n",
    "    train_recall_pos=tp/(tp+fn)\n",
    "    train_f1_pos=2*train_precision_pos*train_recall_pos/(train_precision_pos+train_recall_pos)\n",
    "#     train_balanced_accuracy = (train_recall+(tn/(tn+fp)))/2\n",
    "    train_precision_neg=tn/(tn+fn)\n",
    "    train_recall_neg=tn/(tn+fp)\n",
    "    train_f1_neg=2*train_precision_neg*train_recall_neg/(train_precision_neg+train_recall_neg)\n",
    "    train_f1_beta_pos=((1+beta*beta)*train_precision_pos*train_recall_pos)/((1+beta*beta)*train_precision_pos*train_recall_pos+beta*beta*train_precision_pos+train_recall_pos)\n",
    "    \n",
    "#     train_precision_neg=tn/(tn+fn)\n",
    "#     train_recall_neg=tn/(tn+fp)\n",
    "#     train_f1_neg=2*train_precision_pos*train_recall_pos/(train_precision_pos+train_recall_pos)\n",
    "    train_f1_beta_neg=((1+beta*beta)*train_precision_neg*train_recall_neg)/((1+beta*beta)*train_precision_neg*train_recall_neg+beta*beta*train_precision_neg+train_recall_neg)\n",
    "    \n",
    "    train_f1_beta=(train_f1_beta_pos+train_f1_beta_neg)/2\n",
    "    print(f\" check_train_accuracy: {(tp+tn)/(tp+tn+fp+fn)}\")\n",
    "\n",
    "    return [train_loss, train_accuracy, train_precision_pos, train_recall_pos, train_f1_pos, train_f1_beta_pos,train_precision_neg, train_recall_neg, train_f1_neg, train_f1_beta_neg, train_f1_beta]  \n",
    "    \n",
    "    #         loop.set_description(\"Epoch: {}\".format(epoch))\n",
    "    #         loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06ba14de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLM(model, train_dataloader):\n",
    "\n",
    "    set_seed(42)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-05)\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    train_loss = []\n",
    "    for batch in train_dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels_MLM'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss   \n",
    "        train_loss.append(loss.item())\n",
    "        b_probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(b_probabilities, dim=2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss = np.mean(train_loss)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27d0edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_loss)\n",
    "# print(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e64980bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify loss function\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# model.to(device)\n",
    "# prob_tp=[]\n",
    "# prob_fp=[]\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    pred_label=[]\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        b_attn_mask = batch['attention_mask'].to(device)\n",
    "        b_pids = batch['position_ids'].to(device)\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_attn_mask, position_ids= b_pids,labels=b_labels)\n",
    "\n",
    "        # Compute loss\n",
    "#         loss=torch.nn.functional.cross_entropy(outputs.logits.view(-1, tokenizer.vocab_size), b_labels.view(-1))\n",
    "        loss = outputs.loss\n",
    "        val_loss.append(loss.item())\n",
    "        b_probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "#         pred = torch.argmax( scores[0][7]).item()\n",
    "#         print(\"predicted token:\", pred, tokenizer.convert_ids_to_tokens([pred])  )\n",
    "#         print(NLLLos( logSoftmax(torch.unsqueeze(scores[0][7], 0)), torch.tensor([pred]))) #the same as F.cross_entropy(scores.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
    "#         print(b_probabilities)\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(b_probabilities, dim=1)\n",
    "#         print(\"preds: \", preds[0])\n",
    "#         print(\"labels: \", b_labels[0])\n",
    "        # Calculate the accuracy rate\n",
    "        for i in range(len(b_labels)):\n",
    "            prob.append(b_probabilities[i][1].item())\n",
    "            if preds[i].item() == 1 and b_labels[i].item() == 1: \n",
    "                tp+=1\n",
    "                pred_label.append(1)\n",
    "#                 print(f\"ground_truth: {b_labels[i].item()} tp: {preds[i].item()} prob:{b_probabilities[i][1]}\")\n",
    "#                 prob_tp.append(b_probabilities[i][1].item())\n",
    "            elif preds[i].item() == 1 and b_labels[i].item() == 0:\n",
    "                fp+=1\n",
    "                pred_label.append(1)\n",
    "#                 print(f\"ground_truth: {b_labels[i].item()} fp: {preds[i].item()} prob:{b_probabilities[i][1]}\")\n",
    "#                 prob_fp.append(b_probabilities[i][1].item())\n",
    "            elif preds[i].item() == 0 and b_labels[i].item() == 0:\n",
    "                tn+=1\n",
    "                pred_label.append(0)\n",
    "#                 print(f\"ground_truth: {b_labels[i].item()} tn: {preds[i].item()} prob:{b_probabilities[i][1]}\")\n",
    "            elif preds[i].item() == 0 and b_labels[i].item() == 1:\n",
    "                fn+=1 \n",
    "                pred_label.append(0)\n",
    "#                 print(f\"ground_truth: {b_labels[i].item()} fn: {preds[i].item()} prob:{b_probabilities[i][1]}\")\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    beta=0.5\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    try:\n",
    "        val_precision_pos=tp/(tp+fp)\n",
    "    except ZeroDivisionError:\n",
    "        val_precision_pos=0\n",
    "    try:\n",
    "        val_recall_pos=tp/(tp+fn)\n",
    "    except ZeroDivisionError:\n",
    "        val_recall_pos=0\n",
    "    try:\n",
    "        val_f1_pos=2*val_precision_pos*val_recall_pos/(val_precision_pos+val_recall_pos)\n",
    "    except ZeroDivisionError:\n",
    "        val_f1_pos=0\n",
    "    try:\n",
    "        val_f1_beta_pos=((1+beta*beta)*val_precision_pos*val_recall_pos)/((1+beta*beta)*val_precision_pos*val_recall_pos+beta*beta*val_precision_pos+val_recall_pos)\n",
    "    except ZeroDivisionError:\n",
    "        val_f1_beta_pos=0\n",
    "#     try:\n",
    "#         val_balanced_accuracy = (val_recall+(tn/(tn+fp)))/2\n",
    "#     except ZeroDivisionError:\n",
    "#         val_balanced_accuracy =0\n",
    "    try:\n",
    "        val_precision_neg=tn/(tn+fn)\n",
    "    except ZeroDivisionError:\n",
    "        val_precision_neg=0\n",
    "    try:\n",
    "        val_recall_neg=tn/(tn+fp)\n",
    "    except ZeroDivisionError:\n",
    "        val_recall_neg=0\n",
    "    try:\n",
    "        val_f1_neg=2*val_precision_neg*val_recall_neg/(val_precision_neg+val_recall_neg)\n",
    "    except ZeroDivisionError:\n",
    "        val_f1_neg=0\n",
    "    try:\n",
    "        val_f1_beta_neg=((1+beta*beta)*val_precision_neg*val_recall_neg)/((1+beta*beta)*val_precision_neg*val_recall_neg+beta*beta*val_precision_neg+val_recall_neg)\n",
    "    except ZeroDivisionError:\n",
    "        val_f1_beta_neg=0\n",
    "    try:\n",
    "        val_f1_beta=(val_f1_beta_pos+val_f1_beta_neg)/2\n",
    "    except ZeroDivisionError:\n",
    "        val_f1_beta=0\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        print(f\" check_val_accuracy: {(tp+tn)/(tp+tn+fp+fn)}\")\n",
    "    except ZeroDivisionError:\n",
    "#         check_val_accuracy=0\n",
    "        print(\"ZeroDivisionError\")\n",
    "\n",
    "    return [val_loss, val_accuracy, val_precision_pos, val_recall_pos, val_f1_pos, val_f1_beta_pos,val_precision_neg, val_recall_neg, val_f1_neg, val_f1_beta_neg, val_f1_beta, pred_label] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7c66b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=[8, 12]\n",
    "# learning_rate=[2e-5, 3e-5, 5e-5]\n",
    "epochs=[1,2,3,4,5,6,7,8,9,10]\n",
    "result_train=[[[] for bs in batch_size] for e in epochs]\n",
    "result_val=[[[] for bs in batch_size] for e in epochs]\n",
    "#result_val[e][lr][bs]=[metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f4232b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa975c67",
   "metadata": {},
   "source": [
    "# ----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e39c876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss, val_accuracy, val_precision_pos, val_recall_pos, val_f1_pos, val_f1_beta_pos, ...negs, f1_beta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9642058165548099\n",
      " check_val_accuracy: 0.6888111888111889\n",
      "batch_size: 8 epoch: 1 result_train: [0.09952109833798421, 96.42172523961662, 0.9599430018999366, 0.9688398849472675, 0.9643709241291554, 0.4902404709154713, 0.9685483870967742, 0.9595717481623521, 0.9640391716166318, 0.4915442921925904, 0.4908923815540308] result_val: [0.9813734286775192, 69.09722222222223, 0.717741935483871, 0.6223776223776224, 0.6666666666666666, 0.41051660516605165, 0.6666666666666666, 0.7552447552447552, 0.7081967213114753, 0.40570999248685197, 0.4081132988264518]\n",
      "best_model_val_acc: 69.09722222222223\n",
      " best_model_val_f1: 0.6666666666666666\n",
      " best_model_val_f1_beta: 0.4081132988264518\n",
      " check_train_accuracy: 0.9790667945030361\n",
      " check_val_accuracy: 0.6538461538461539\n",
      "batch_size: 8 epoch: 2 result_train: [0.06733511998665893, 97.90734824281151, 0.9838605551969012, 0.9741131351869607, 0.9789625823028746, 0.49543252820129385, 0.9743670886075949, 0.9840204538191115, 0.9791699793289872, 0.49399948658708764, 0.49471600739419075] result_val: [1.619316913989476, 65.625, 0.7156862745098039, 0.5104895104895105, 0.5959183673469388, 0.398471615720524, 0.6195652173913043, 0.7972027972027972, 0.6972477064220184, 0.39337474120082816, 0.39592317846067604]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9925695110258869\n",
      " check_val_accuracy: 0.6503496503496503\n",
      "batch_size: 8 epoch: 3 result_train: [0.08238359038015462, 99.25718849840256, 0.9917052161429255, 0.9934483860658357, 0.9925760357627524, 0.49800541501786316, 0.9934368496878502, 0.991690635985938, 0.9925629748100759, 0.4982657845719058, 0.4981355997948845] result_val: [1.7759006426462696, 65.04629629629629, 0.6387096774193548, 0.6923076923076923, 0.6644295302013423, 0.3934817170111288, 0.6641221374045801, 0.6083916083916084, 0.635036496350365, 0.3947368421052632, 0.39410927955819597]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.980664749121125\n",
      " check_val_accuracy: 0.7027972027972028\n",
      "batch_size: 8 epoch: 4 result_train: [0.04814340802755344, 98.06709265175719, 0.9859450726978999, 0.9752317034196228, 0.9805591259640103, 0.49591276225765035, 0.9754979449889346, 0.986097794822627, 0.9807692307692307, 0.4943364788439047, 0.49512462055077755] result_val: [1.5239459235453978, 70.25462962962962, 0.6746987951807228, 0.7832167832167832, 0.7249190938511327, 0.4096561814191661, 0.7416666666666667, 0.6223776223776224, 0.67680608365019, 0.4166666666666667, 0.4131614240429164]\n",
      "best_model_val_acc: 70.25462962962962\n",
      " best_model_val_f1: 0.7249190938511327\n",
      " best_model_val_f1_beta: 0.4131614240429164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.8835890060722276\n",
      " check_val_accuracy: 0.5\n",
      "batch_size: 8 epoch: 5 result_train: [0.12588651232236733, 88.33865814696486, 0.889754830329599, 0.8756791307126878, 0.882660868164613, 0.47003122105190925, 0.8776152273084789, 0.8914988814317674, 0.884502576298058, 0.4681861667310049, 0.46910869389145704] result_val: [0.6931833277146021, 50.34722222222222, 0, 0.0, 0, 0, 0.5, 1.0, 0.6666666666666666, 0.35714285714285715, 0.17857142857142858]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.955017577500799\n",
      " check_val_accuracy: 0.541958041958042\n",
      "batch_size: 8 epoch: 6 result_train: [0.11312382634805021, 95.50319488817891, 0.9412676274600961, 0.9705976350271652, 0.9557076547871921, 0.486386931454196, 0.9696519874649513, 0.9394375199744327, 0.9543056570083597, 0.4906935981971454, 0.4885402648256707] result_val: [1.6129435565736558, 53.81944444444444, 0.5223880597014925, 0.9790209790209791, 0.681265206812652, 0.36553524804177545, 0.8333333333333334, 0.1048951048951049, 0.1863354037267081, 0.2586206896551724, 0.31207796884847394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9761105784595717\n",
      " check_val_accuracy: 0.5\n",
      "batch_size: 8 epoch: 7 result_train: [0.05704623016371364, 97.59584664536742, 0.9755786113328013, 0.9766698625759028, 0.976123931965184, 0.4938750444422897, 0.9766437370020796, 0.9755512943432406, 0.9760972100087936, 0.49403596225743274, 0.4939555033498612] result_val: [0.6991437822580338, 50.34722222222222, 0, 0.0, 0, 0, 0.5, 1.0, 0.6666666666666666, 0.35714285714285715, 0.17857142857142858]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9753116011505273\n",
      " check_val_accuracy: 0.6258741258741258\n",
      "batch_size: 8 epoch: 8 result_train: [0.043337519131410006, 97.53194888178913, 0.9668811803484539, 0.9843400447427293, 0.9755325045530129, 0.4924690607911484, 0.9840520748576078, 0.9662831575583254, 0.9750866725792148, 0.4950632849213237, 0.49376617285623603] result_val: [1.8460651330856814, 62.73148148148149, 0.6111111111111112, 0.6923076923076923, 0.6491803278688525, 0.3849144634525661, 0.6451612903225806, 0.5594405594405595, 0.599250936329588, 0.38498556304138587, 0.384950013246976]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9841802492809204\n",
      " check_val_accuracy: 0.6363636363636364\n",
      "batch_size: 8 epoch: 9 result_train: [0.03772135670527912, 98.4185303514377, 0.9922027290448343, 0.9760306807286673, 0.9840502658289029, 0.4972159812445052, 0.9764150943396226, 0.9923298178331735, 0.9843081312410842, 0.49483648881239245, 0.49602623502844884] result_val: [1.781446352820947, 63.77314814814815, 0.7010309278350515, 0.4755244755244755, 0.5666666666666665, 0.39035591274397247, 0.6031746031746031, 0.7972027972027972, 0.6867469879518072, 0.38801906058543223, 0.3891874866647024]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9923298178331735\n",
      " check_val_accuracy: 0.5629370629370629\n",
      "batch_size: 8 epoch: 10 result_train: [0.03313920238394818, 99.2332268370607, 0.9923298178331735, 0.9923298178331735, 0.9923298178331735, 0.4980750721847931, 0.9923298178331735, 0.9923298178331735, 0.9923298178331735, 0.4980750721847931, 0.4980750721847931] result_val: [2.339109807715027, 55.90277777777778, 0.5387931034482759, 0.8741258741258742, 0.6666666666666669, 0.36851415094339623, 0.6666666666666666, 0.2517482517482518, 0.3654822335025381, 0.3339517625231911, 0.3512329567332937]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9678012144455097\n",
      " check_val_accuracy: 0.5384615384615384\n",
      "batch_size: 12 epoch: 1 result_train: [0.09311373459178801, 96.78012144455097, 0.9610962356276579, 0.975071907957814, 0.9680336321091457, 0.49079853291293996, 0.9747040700502676, 0.9605305209332055, 0.9675653923541246, 0.4928584313146718, 0.49182848211380586] result_val: [1.427218398079276, 53.61111111111111, 0.5219123505976095, 0.916083916083916, 0.66497461928934, 0.3634850166481687, 0.6571428571428571, 0.16083916083916083, 0.2584269662921348, 0.2889447236180905, 0.32621487013312955]\n",
      " check_train_accuracy: 0.9914509427932247\n",
      " check_val_accuracy: 0.6398601398601399\n",
      "batch_size: 12 epoch: 2 result_train: [0.03004697056806759, 99.14509427932248, 0.9912154607890113, 0.991690635985938, 0.9914529914529916, 0.49781813514727585, 0.9916866506794564, 0.9912112496005113, 0.9914488931511227, 0.4978890083958069, 0.4978535717715414] result_val: [1.6348710121043648, 63.888888888888886, 0.5909090909090909, 0.9090909090909091, 0.7162534435261708, 0.38852361028093246, 0.803030303030303, 0.3706293706293706, 0.507177033492823, 0.3943452380952381, 0.3914344241880853]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9943272611057846\n",
      " check_val_accuracy: 0.6153846153846154\n",
      "batch_size: 12 epoch: 3 result_train: [0.07901744830317965, 99.43272611057847, 0.9940904008944258, 0.994566954298498, 0.9943286204968448, 0.4985421806414405, 0.994564348521183, 0.9940875679130713, 0.9943259010628946, 0.4986134042928362, 0.49857779246713835] result_val: [1.8656094189112384, 61.388888888888886, 0.582089552238806, 0.8181818181818182, 0.680232558139535, 0.3818537859007833, 0.6941176470588235, 0.4125874125874126, 0.5175438596491228, 0.3791773778920309, 0.3805155818964071]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9972834771492489\n",
      " check_val_accuracy: 0.5559440559440559\n",
      "batch_size: 12 epoch: 4 result_train: [0.02419319727085486, 99.7283477149249, 0.9974424552429667, 0.99712368168744, 0.9972830429918491, 0.4993438110175731, 0.9971246006389777, 0.9974432726110578, 0.9972839111679183, 0.49929609010046705, 0.49931995055902006] result_val: [3.0964165570391438, 55.208333333333336, 0.5305343511450382, 0.972027972027972, 0.6864197530864197, 0.3685047720042418, 0.8333333333333334, 0.13986013986013987, 0.23952095808383234, 0.29498525073746307, 0.33174501137085244]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9975231703419623\n",
      " check_val_accuracy: 0.6538461538461539\n",
      "batch_size: 12 epoch: 5 result_train: [0.014275165230253377, 99.75231703419622, 0.997602684992808, 0.9974432726110578, 0.9975229724330803, 0.49939196108550943, 0.9974436810992171, 0.9976030680728667, 0.9975233682192218, 0.4993680909948967, 0.49938002604020304] result_val: [1.8937818021901573, 65.1388888888889, 0.602803738317757, 0.9020979020979021, 0.7226890756302521, 0.39233576642335766, 0.8055555555555556, 0.40559440559440557, 0.5395348837209302, 0.40221914008321774, 0.39727745325328767]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9988814317673378\n",
      " check_val_accuracy: 0.5769230769230769\n",
      "batch_size: 12 epoch: 6 result_train: [0.008563986203150733, 99.88814317673378, 0.9990409207161125, 0.9987216363055289, 0.9988812529966437, 0.4997441310049255, 0.9987220447284345, 0.9990412272291467, 0.9988816104809074, 0.49969628185044274, 0.4997202064276841] result_val: [2.917758474838289, 57.43055555555555, 0.55, 0.8461538461538461, 0.6666666666666667, 0.3716216216216216, 0.6666666666666666, 0.3076923076923077, 0.42105263157894735, 0.3508771929824561, 0.36124940730203886]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.99928092042186\n",
      " check_val_accuracy: 0.534965034965035\n",
      "batch_size: 12 epoch: 7 result_train: [0.0048603387868444365, 99.92809204218601, 0.9993607159980822, 0.9992010226909556, 0.9992808629644426, 0.49983213697622747, 0.9992011503435053, 0.9993608181527645, 0.9992809778700966, 0.49980819640687935, 0.4998201666915534] result_val: [3.647672447360492, 53.125, 0.5189393939393939, 0.958041958041958, 0.6732186732186732, 0.36358811040339706, 0.7272727272727273, 0.11188811188811189, 0.19393939393939394, 0.2572347266881029, 0.31041141854574994]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9988015340364333\n",
      " check_val_accuracy: 0.534965034965035\n",
      "batch_size: 12 epoch: 8 result_train: [0.005807205649406929, 99.88015340364333, 0.9988812529966438, 0.9987216363055289, 0.9988014382740712, 0.4997121657925035, 0.9987218405496086, 0.9988814317673378, 0.9988016297834944, 0.49968824441637755, 0.4997002051044405] result_val: [2.9970158797805198, 53.194444444444436, 0.5201612903225806, 0.9020979020979021, 0.659846547314578, 0.36235955056179775, 0.631578947368421, 0.16783216783216784, 0.26519337016574585, 0.28915662650602414, 0.32575808853391097]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9944870565675935\n",
      " check_val_accuracy: 0.6608391608391608\n",
      "batch_size: 12 epoch: 9 result_train: [0.013387110102758974, 99.44870565675934, 0.9967892117514849, 0.9921700223713646, 0.9944742532233523, 0.4989633391729215, 0.9922061396532528, 0.9968040907638223, 0.9944998007174173, 0.49827465013738903, 0.49861899465515525] result_val: [1.8005046745141347, 66.04166666666667, 0.6337209302325582, 0.7622377622377622, 0.692063492063492, 0.3960755813953489, 0.7017543859649122, 0.5594405594405595, 0.622568093385214, 0.40040040040040037, 0.3982379908978746]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./sentiment/model/sentiment_25/temp were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sentiment/model/sentiment_25/temp and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9905720677532758\n",
      " check_val_accuracy: 0.6468531468531469\n",
      "batch_size: 12 epoch: 10 result_train: [0.013927917674307847, 99.05720677532757, 0.9978916639636718, 0.9832214765100671, 0.9905022537025112, 0.4987274466257073, 0.9834645669291339, 0.9979226589964845, 0.9906408629441623, 0.49655709810282594, 0.4976422723642666] result_val: [1.9592151679098606, 64.65277777777777, 0.6296296296296297, 0.7132867132867133, 0.6688524590163935, 0.3920061491160646, 0.6693548387096774, 0.5804195804195804, 0.6217228464419474, 0.3937381404174573, 0.3928721447667609]\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "# print(\"val_loss, val_accuracy, val_precision_pos, val_recall_pos, val_f1_pos, val_f1_beta_pos, ...negs, f1_beta\")\n",
    "# best_model_val_acc=-1\n",
    "# best_model_val_f1=-1\n",
    "# best_model_val_f1_beta=-1\n",
    "# for bs in range(len(batch_size)):\n",
    "#     train_dataloader = torch.utils.data.DataLoader(\n",
    "#         train_dataset,\n",
    "#         batch_size=batch_size[bs],\n",
    "#         #shuffle=True\n",
    "#     )\n",
    "#     train_dataloader_MLM = torch.utils.data.DataLoader(\n",
    "#         train_dataset_MLM,\n",
    "#         batch_size=batch_size[bs],\n",
    "#         #shuffle=True\n",
    "#     )\n",
    "#     val_dataloader = torch.utils.data.DataLoader(\n",
    "#         val_dataset,\n",
    "#         batch_size=batch_size[bs],\n",
    "#     )\n",
    "#     torch.cuda.empty_cache() \n",
    "#     model = BertForSequenceClassification.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "#     model.to(device)\n",
    "#     for e in range(len(epochs)): \n",
    "#         if e==0 or e==1:\n",
    "#             with open(\"sentiment_25_result_train.txt\", \"wb\") as ft:   #Pickling\n",
    "#                 result_train[e][bs]=train(model, train_dataloader)\n",
    "#                 pickle.dump(result_train, ft)\n",
    "#             ft.close()\n",
    "#             with open(\"sentiment_25_result_val.txt\", \"wb\") as fv:   #Pickling\n",
    "#                 result_val[e][bs]=evaluate(model, val_dataloader)\n",
    "#                 pickle.dump(result_val, fv)\n",
    "#             fv.close()\n",
    "#         else:\n",
    "#             model = BertForMaskedLM.from_pretrained(\"./sentiment/model/sentiment_25/temp\")\n",
    "#             model.to(device)\n",
    "#             #MLM loss\n",
    "#             train_MLM_loss=train_MLM(model, train_dataloader_MLM)\n",
    "#             model.save_pretrained(save_directory='./sentiment/model/sentiment_25/temp')\n",
    "#             model = BertForSequenceClassification.from_pretrained('./sentiment/model/sentiment_25/temp')\n",
    "#             model.to(device)\n",
    "#             with open(\"sentiment_25_result_train.txt\", \"wb\") as ft:   #Pickling\n",
    "#                 result_train[e][bs]=train(model, train_dataloader)\n",
    "#                 result_train[e][bs][0]=(result_train[e][bs][0]+train_MLM_loss)/2\n",
    "#                 pickle.dump(result_train, ft)\n",
    "#             ft.close()\n",
    "#             with open(\"sentiment_25_result_val.txt\", \"wb\") as fv:   #Pickling\n",
    "#                 result_val[e][bs]=evaluate(model, val_dataloader)\n",
    "#                 pickle.dump(result_val, fv)\n",
    "#             fv.close()\n",
    "#         model.save_pretrained(save_directory='./sentiment/model/sentiment_25/temp') \n",
    "             \n",
    "#         print(f\"batch_size: {batch_size[bs]} epoch: {epochs[e]} result_train: {result_train[e][bs]} result_val: {result_val[e][bs]}\")\n",
    "# #             if(e>1 and result_val[e][lr][bs][5]<result_val[e-1][lr][bs][5]):\n",
    "# #                 print(\"Early stopping\")\n",
    "# #                 break\n",
    "#         if result_val[e][bs][1] > best_model_val_acc:\n",
    "#             best_model_val_acc=result_val[e][bs][1]\n",
    "# #                 !rmdir model_prompt1_MLM\n",
    "#             print(f\"best_model_val_acc: {best_model_val_acc}\")\n",
    "#             model.save_pretrained(save_directory='./sentiment/model/microf1/sentiment_25/saved_model')\n",
    "        \n",
    "#         if result_val[e][bs][4] > best_model_val_f1:\n",
    "#             best_model_val_f1=result_val[e][bs][4]\n",
    "# #                 !rmdir model_prompt1_MLM\n",
    "#             print(f\" best_model_val_f1: {best_model_val_f1}\")\n",
    "#             model.save_pretrained(save_directory='./sentiment/model/f1/sentiment_25/saved_model')\n",
    "#         if result_val[e][bs][10] > best_model_val_f1_beta:\n",
    "#             best_model_val_f1_beta=result_val[e][bs][10]\n",
    "# #                 !rmdir model_prompt1_MLM\n",
    "#             print(f\" best_model_val_f1_beta: {best_model_val_f1_beta}\")\n",
    "#             model.save_pretrained(save_directory='./sentiment/model/f1beta/sentiment_25/saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c846f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"sentiment_25_result_train.txt\", \"wb\") as ft:   #Pickling\n",
    "#     pickle.dump(result_train, ft)\n",
    "    \n",
    "# with open(\"sentiment_25_result_val.txt\", \"wb\") as fv:   #Pickling\n",
    "#     pickle.dump(result_val, fv)\n",
    "\n",
    "# result_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b4b4196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"sentiment_25_result_train.txt\", \"wb\") as ft:   #Pickling\n",
    "#     result_train = pickle.load(ft)\n",
    "    \n",
    "# with open(\"sentiment_25_result_val.txt\", \"wb\") as fv:   #Pickling\n",
    "#     result_val = pickle.load(fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ce460bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70.25462962962962, 4, 8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #pick the best model\n",
    "# max_result=-1\n",
    "# for e in range(len(epochs)):\n",
    "#     for bs in range(len(batch_size)):\n",
    "#         if(len(result_val[e][bs])!=0):\n",
    "#             if(result_val[e][bs][1]>max_result):\n",
    "#                 max_result=result_val[e][bs][1]\n",
    "#                 max_e=epochs[e]\n",
    "#                 max_bs=batch_size[bs]\n",
    "                    \n",
    "# max_result,max_e,max_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "049d91ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=8,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8407af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=12,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6335d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_train_dataloader = torch.utils.data.DataLoader(\n",
    "                noisy_train_dataset,\n",
    "                batch_size=8,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c454eb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "model = BertForSequenceClassification.from_pretrained(\"../../sentiment/sentiment/model/microf1/sentiment_25/saved_model\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f88e8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_val_accuracy: 0.8269553450608931\n",
      "batch_size: 8 epoch: 4 result_test: [0.7597893044888747, 82.6911976911977, 1.0, 0.8269553450608931, 0.9052824934079934, 0.48975169416003644, 0.0, 0, 0, 0, 0.24487584708001822]\n"
     ]
    }
   ],
   "source": [
    "prob=[]\n",
    "prob_tp=[]\n",
    "prob_fp=[]\n",
    "max_result_noisy_train=evaluate(model, noisy_train_dataloader)\n",
    "print(f\"batch_size: {8} epoch: {4} result_test: {max_result_noisy_train[:-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c972d87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_val_accuracy: 0.6083916083916084\n",
      "batch_size: 8 epoch: 4 result_test: [2.0935775167632125, 60.64814814814815, 0.5701357466063348, 0.8811188811188811, 0.6923076923076924, 0.38020519010259507, 0.7384615384615385, 0.3356643356643357, 0.4615384615384616, 0.37325038880248834, 0.3767277894525417, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "model = BertForSequenceClassification.from_pretrained(\"../../sentiment/sentiment/model/microf1/sentiment_25/saved_model\")\n",
    "model.to(device)\n",
    "prob=[]\n",
    "prob_tp=[]\n",
    "prob_fp=[]\n",
    "max_result_test=evaluate(model, val_dataloader)\n",
    "print(f\"batch_size: {8} epoch: {4} result_test: {max_result_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6746844",
   "metadata": {},
   "source": [
    "Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "462f94b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9955599, 0.9962175)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(prob_tp)\n",
    "plt.xlim([0.9955599, 0.9962175])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e7a98d35",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9961245656013489,\n",
       " 0.9961230158805847,\n",
       " 0.9961265921592712,\n",
       " 0.9961764812469482,\n",
       " 0.9961644411087036,\n",
       " 0.9961687922477722,\n",
       " 0.9961638450622559,\n",
       " 0.9961416125297546,\n",
       " 0.9961251616477966,\n",
       " 0.9961766004562378,\n",
       " 0.9961219429969788,\n",
       " 0.9961282014846802,\n",
       " 0.9961704611778259,\n",
       " 0.996163010597229,\n",
       " 0.9961189031600952,\n",
       " 0.9961689114570618,\n",
       " 0.99610435962677,\n",
       " 0.9962047934532166,\n",
       " 0.9961407780647278,\n",
       " 0.996204674243927,\n",
       " 0.9961159229278564,\n",
       " 0.9961208701133728,\n",
       " 0.9962009787559509,\n",
       " 0.9960739612579346,\n",
       " 0.9960899353027344,\n",
       " 0.9961472749710083,\n",
       " 0.9961703419685364,\n",
       " 0.996146559715271,\n",
       " 0.9961801767349243,\n",
       " 0.996126115322113,\n",
       " 0.9961650371551514,\n",
       " 0.996137797832489,\n",
       " 0.9962095022201538,\n",
       " 0.9961007833480835,\n",
       " 0.9962131977081299,\n",
       " 0.9961481094360352,\n",
       " 0.9961416125297546,\n",
       " 0.996155321598053,\n",
       " 0.9961358904838562,\n",
       " 0.9961186647415161,\n",
       " 0.9961981177330017,\n",
       " 0.9960430860519409,\n",
       " 0.9961714148521423,\n",
       " 0.9956908822059631,\n",
       " 0.9961446523666382,\n",
       " 0.9961621761322021,\n",
       " 0.9961292743682861,\n",
       " 0.996029257774353,\n",
       " 0.996159553527832,\n",
       " 0.9961589574813843,\n",
       " 0.9961036443710327,\n",
       " 0.996136486530304,\n",
       " 0.9959491491317749,\n",
       " 0.9960959553718567,\n",
       " 0.9960894584655762,\n",
       " 0.9961231350898743,\n",
       " 0.996049702167511,\n",
       " 0.9961152076721191,\n",
       " 0.9546504616737366,\n",
       " 0.9960933327674866,\n",
       " 0.9961321353912354,\n",
       " 0.9959690570831299,\n",
       " 0.9960511326789856,\n",
       " 0.9961179494857788,\n",
       " 0.9961062073707581,\n",
       " 0.9961036443710327,\n",
       " 0.9959740042686462,\n",
       " 0.9961186647415161,\n",
       " 0.9960983991622925,\n",
       " 0.9961110949516296,\n",
       " 0.9960339665412903,\n",
       " 0.9959806203842163,\n",
       " 0.996056318283081,\n",
       " 0.9960401058197021,\n",
       " 0.9960009455680847,\n",
       " 0.9957234859466553,\n",
       " 0.9959771037101746,\n",
       " 0.9961020946502686,\n",
       " 0.9961209893226624,\n",
       " 0.995780348777771,\n",
       " 0.9958123564720154,\n",
       " 0.9960916638374329,\n",
       " 0.9961267113685608,\n",
       " 0.996116042137146,\n",
       " 0.9961447715759277,\n",
       " 0.9958042502403259,\n",
       " 0.9960355162620544,\n",
       " 0.99610435962677,\n",
       " 0.9960822463035583,\n",
       " 0.9961023330688477,\n",
       " 0.9961130619049072,\n",
       " 0.9961052536964417,\n",
       " 0.9961373209953308,\n",
       " 0.9959965944290161,\n",
       " 0.9960997104644775,\n",
       " 0.996100902557373,\n",
       " 0.9960986375808716,\n",
       " 0.9961169958114624,\n",
       " 0.9961073994636536,\n",
       " 0.9961190223693848,\n",
       " 0.9960445165634155,\n",
       " 0.9961179494857788,\n",
       " 0.996084451675415,\n",
       " 0.9960516095161438,\n",
       " 0.9961268305778503,\n",
       " 0.9961286783218384,\n",
       " 0.9961211085319519,\n",
       " 0.9960542917251587,\n",
       " 0.9961095452308655,\n",
       " 0.9961372017860413,\n",
       " 0.9961345195770264,\n",
       " 0.9961047172546387,\n",
       " 0.9959761500358582,\n",
       " 0.9961456060409546,\n",
       " 0.995974600315094,\n",
       " 0.996100902557373,\n",
       " 0.9958446621894836,\n",
       " 0.9961464405059814,\n",
       " 0.9959617257118225,\n",
       " 0.9961305856704712,\n",
       " 0.9960998296737671,\n",
       " 0.9960401058197021,\n",
       " 0.9961464405059814,\n",
       " 0.9961079955101013,\n",
       " 0.9961133003234863,\n",
       " 0.9960736036300659]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b4f62a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9955599, 0.9962175)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(prob_fp)\n",
    "plt.xlim([0.9955599, 0.9962175])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b0e1b8f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9961422085762024,\n",
       " 0.9961743354797363,\n",
       " 0.9960947632789612,\n",
       " 0.9961426854133606,\n",
       " 0.9961559176445007,\n",
       " 0.9961352348327637,\n",
       " 0.9961426854133606,\n",
       " 0.9959348440170288,\n",
       " 0.9961155652999878,\n",
       " 0.9961119294166565,\n",
       " 0.9961297512054443,\n",
       " 0.9961354732513428,\n",
       " 0.9961649179458618,\n",
       " 0.9960930943489075,\n",
       " 0.9961040019989014,\n",
       " 0.9961744546890259,\n",
       " 0.9960312247276306,\n",
       " 0.9960792660713196,\n",
       " 0.9957410097122192,\n",
       " 0.9961077570915222,\n",
       " 0.9962174296379089,\n",
       " 0.9958977103233337,\n",
       " 0.9960916638374329,\n",
       " 0.9960500597953796,\n",
       " 0.9961090683937073,\n",
       " 0.9961122870445251,\n",
       " 0.9961113333702087,\n",
       " 0.996134877204895,\n",
       " 0.9961192607879639,\n",
       " 0.9960605502128601,\n",
       " 0.9958386421203613,\n",
       " 0.9958581328392029,\n",
       " 0.9955679774284363,\n",
       " 0.9960039258003235,\n",
       " 0.9909679293632507,\n",
       " 0.9961066842079163,\n",
       " 0.9957489967346191,\n",
       " 0.9957013726234436,\n",
       " 0.9959437251091003,\n",
       " 0.9959115982055664,\n",
       " 0.9961048364639282,\n",
       " 0.9959555864334106,\n",
       " 0.995999813079834,\n",
       " 0.9961135387420654,\n",
       " 0.985625684261322,\n",
       " 0.9961137771606445,\n",
       " 0.995790421962738,\n",
       " 0.996009349822998,\n",
       " 0.9354114532470703,\n",
       " 0.9959004521369934,\n",
       " 0.99607253074646,\n",
       " 0.9960970282554626,\n",
       " 0.9959934949874878,\n",
       " 0.9958409667015076,\n",
       " 0.9960471987724304,\n",
       " 0.9961268305778503,\n",
       " 0.9956458210945129,\n",
       " 0.9960643649101257,\n",
       " 0.9960980415344238,\n",
       " 0.9959989786148071,\n",
       " 0.9960838556289673,\n",
       " 0.9961232542991638,\n",
       " 0.9961143732070923,\n",
       " 0.9961280822753906,\n",
       " 0.9959618449211121,\n",
       " 0.9960493445396423,\n",
       " 0.9959787130355835,\n",
       " 0.9957993626594543,\n",
       " 0.9961121678352356,\n",
       " 0.996037483215332,\n",
       " 0.9956361651420593,\n",
       " 0.9960440397262573,\n",
       " 0.9958979487419128,\n",
       " 0.996110737323761,\n",
       " 0.9961189031600952,\n",
       " 0.9957907795906067,\n",
       " 0.9961112141609192,\n",
       " 0.9957588315010071,\n",
       " 0.996119499206543,\n",
       " 0.9959744811058044,\n",
       " 0.9960967898368835,\n",
       " 0.9961069226264954,\n",
       " 0.9960885047912598,\n",
       " 0.9960512518882751,\n",
       " 0.996122419834137,\n",
       " 0.99617600440979,\n",
       " 0.9960667490959167,\n",
       " 0.9960978031158447,\n",
       " 0.9958130717277527,\n",
       " 0.9960833787918091,\n",
       " 0.9959834814071655,\n",
       " 0.995964527130127,\n",
       " 0.995966911315918,\n",
       " 0.9961116909980774,\n",
       " 0.9959191679954529]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cebf74d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n",
      "95\n",
      "0.5701357466063348\n",
      "0.8811188811188811\n",
      "0.6923076923076924\n"
     ]
    }
   ],
   "source": [
    "print(len(prob_tp))\n",
    "print(len(prob_fp))\n",
    "tp=len(prob_tp)\n",
    "fp=len(prob_fp)\n",
    "precision=tp/(tp+fp)\n",
    "recall=tp/143\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06b9beb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "5\n",
      "0.7916666666666666\n",
      "0.13286713286713286\n",
      "0.22754491017964074\n"
     ]
    }
   ],
   "source": [
    "tp=0\n",
    "fp=0\n",
    "\n",
    "for i in range(len(prob_tp)):\n",
    "    if prob_tp[i]>0.99616:\n",
    "        tp+=1\n",
    "for i in range(len(prob_fp)):\n",
    "    if prob_fp[i]>0.99616:\n",
    "        fp+=1\n",
    "print(tp)\n",
    "print(fp)\n",
    "precision=tp/(tp+fp)\n",
    "recall=tp/143\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29a18cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n",
      "286\n"
     ]
    }
   ],
   "source": [
    "print(len(prob_tp)+len(prob_fp))\n",
    "print(len(prob))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31dc35",
   "metadata": {},
   "source": [
    "Unlabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "433ce628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    15278.000000\n",
       "mean         0.995911\n",
       "std          0.004171\n",
       "min          0.578289\n",
       "25%          0.996057\n",
       "50%          0.996115\n",
       "75%          0.996146\n",
       "max          0.996217\n",
       "dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_tp_ser=pd.Series(prob_tp)\n",
    "prob_tp_ser.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91e5ab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.996057, 0.996217)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(prob_tp)\n",
    "plt.xlim([0.996057, 0.996217])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1890486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "55c0f34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18475"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max_result_noisy_train[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13b0b7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "61254b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"test_labels/sentiment_25_cot1.txt\", \"wb\") as f:   #Pickling\n",
    "    sentiment_labels=max_result_noisy_train[-1]\n",
    "    pickle.dump(sentiment_labels, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f32249e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max_result_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "328fb7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"val_labels/sentiment_25_labels.txt\", \"wb\") as f:   #Pickling\n",
    "    sentiment_labels=max_result_test[-1]\n",
    "    pickle.dump(sentiment_labels, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913f4f3",
   "metadata": {},
   "source": [
    "# 2000eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6585820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=12,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "792487c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_val_accuracy: 0.6695\n",
      "batch_size: 12 epoch: 4 result_test: [1.775649806762025, 66.99101796407186, 0.6330390920554855, 0.9270544783010157, 0.7523417010116148, 0.40331003454647707, 0.8091787439613527, 0.3653217011995638, 0.5033809166040571, 0.39430320150659137, 0.3988066180265342, [0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "model = BertForSequenceClassification.from_pretrained(\"../../sentiment/sentiment/model/f1/sentiment_25/saved_model\")\n",
    "model.to(device)\n",
    "prob=[]\n",
    "# prob_tp=[]\n",
    "# prob_fp=[]\n",
    "max_result_test=evaluate(model, test_dataloader)\n",
    "print(f\"batch_size: {12} epoch: {4} result_test: {max_result_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82d9a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"2000/test_labels/sentiment_25_labels.txt\", \"wb\") as f:   #Pickling\n",
    "    sentiment_labels=max_result_test[-1]\n",
    "    pickle.dump(sentiment_labels, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d696087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"2000/test_labels/sentiment_25_prob.txt\", \"wb\") as f:   #Pickling\n",
    "#     sentiment_labels=max_result_test[-1]\n",
    "    pickle.dump(prob, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "443c0e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_val_accuracy: 0.6083916083916084\n",
      "batch_size: 12 epoch: 4 result_test: [2.094836361706257, 60.625, 0.5701357466063348, 0.8811188811188811, 0.6923076923076924, 0.38020519010259507, 0.7384615384615385, 0.3356643356643357, 0.4615384615384616, 0.37325038880248834, 0.3767277894525417, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "model = BertForSequenceClassification.from_pretrained(\"../../sentiment/sentiment/model/f1/sentiment_25/saved_model\")\n",
    "model.to(device)\n",
    "prob=[]\n",
    "# prob_tp=[]\n",
    "# prob_fp=[]\n",
    "max_result_test=evaluate(model, val_dataloader)\n",
    "print(f\"batch_size: {12} epoch: {4} result_test: {max_result_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6208f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"2000/val_labels/sentiment_25_labels.txt\", \"wb\") as f:   #Pickling\n",
    "    sentiment_labels=max_result_test[-1]\n",
    "    pickle.dump(sentiment_labels, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94e2f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"2000/val_labels/sentiment_25_prob.txt\", \"wb\") as f:   #Pickling\n",
    "#     sentiment_labels=max_result_test[-1]\n",
    "    pickle.dump(prob, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a52d341",
   "metadata": {},
   "source": [
    "Connor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ac95af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_val_accuracy: 0.57\n",
      "batch_size: 12 epoch: 4 result_test: [2.5114069514804416, 54.62962962962963, 0.44155844155844154, 1.0, 0.6126126126126126, 0.33203125, 1.0, 0.3484848484848485, 0.5168539325842696, 0.42124542124542125, 0.37663833562271065, [0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "model = BertForSequenceClassification.from_pretrained(\"../../sentiment/sentiment/model/f1/sentiment_25/saved_model\")\n",
    "model.to(device)\n",
    "prob=[]\n",
    "# prob_tp=[]\n",
    "# prob_fp=[]\n",
    "max_result_test=evaluate(model, test_dataloader)\n",
    "print(f\"batch_size: {12} epoch: {4} result_test: {max_result_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78911da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
