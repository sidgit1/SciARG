{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47e5323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (4.9.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (1.20.2)\n",
      "Requirement already satisfied: sacremoses in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: requests in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: filelock in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (4.61.1)\n",
      "Requirement already satisfied: packaging in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from huggingface-hub==0.0.12->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: joblib in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /home/sidhant/anaconda3/envs/sid/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AdamW\n",
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f8abb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#change sheetname\n",
    "# train_df=pd.read_excel('../../data/data.xlsx', sheet_name=0)\n",
    "test_df=pd.read_excel('../../data/data.xlsx', sheet_name=2)\n",
    "val_df=pd.read_excel('../../data/data.xlsx', sheet_name=1)\n",
    "\n",
    "\n",
    "bad_chars = [';', ':', \"(\", \")\", \"-\", \"'\", \"\\\"\", \"_\", \".\", \",\", \" \"]\n",
    "\n",
    "def components(df):\n",
    "    statements = list(df['statements'])\n",
    "    labels = list(df['labels'])\n",
    "    genes = list(df['target_genes'])\n",
    "    return statements, labels, genes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475d02e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-193753be380d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ff6b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target_genes</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>statements</th>\n",
       "      <th>review_label</th>\n",
       "      <th>label</th>\n",
       "      <th>Remarks</th>\n",
       "      <th>labels</th>\n",
       "      <th>connor_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1436</td>\n",
       "      <td>msbA</td>\n",
       "      <td>4979069</td>\n",
       "      <td>coli conclude that the conformational transiti...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>ABC transporter</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>343</td>\n",
       "      <td>CpxR</td>\n",
       "      <td>5063474</td>\n",
       "      <td>The results showed that, apart from the previo...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1382</td>\n",
       "      <td>mphI</td>\n",
       "      <td>5760710</td>\n",
       "      <td>MphI shares high sequence identity (94%) to ho...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Note that the actual conclusion of the linked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>187</td>\n",
       "      <td>cpxA</td>\n",
       "      <td>3319533</td>\n",
       "      <td>The disruption at cpxAR operon was confirmed w...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>plasmid</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1854</td>\n",
       "      <td>sdiA</td>\n",
       "      <td>2812512</td>\n",
       "      <td>It was recently discovered that S . Typhimuriu...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 target_genes    pmcid  \\\n",
       "0        1436         msbA  4979069   \n",
       "1         343         CpxR  5063474   \n",
       "2        1382         mphI  5760710   \n",
       "3         187         cpxA  3319533   \n",
       "4        1854         sdiA  2812512   \n",
       "\n",
       "                                          statements  review_label  label  \\\n",
       "0  coli conclude that the conformational transiti...          -1.0      1   \n",
       "1  The results showed that, apart from the previo...          -1.0     -1   \n",
       "2  MphI shares high sequence identity (94%) to ho...           1.0      1   \n",
       "3  The disruption at cpxAR operon was confirmed w...          -1.0      1   \n",
       "4  It was recently discovered that S . Typhimuriu...          -1.0     -1   \n",
       "\n",
       "            Remarks  labels                                       connor_notes  \n",
       "0  ABC transporter        0                                                NaN  \n",
       "1               NaN       0                                                NaN  \n",
       "2               NaN       0  Note that the actual conclusion of the linked ...  \n",
       "3           plasmid       0                                                NaN  \n",
       "4               NaN       0                                                NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1fde700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_genes</th>\n",
       "      <th>statements</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDM-1</td>\n",
       "      <td>5% in Indian and Pakistan hospitals . In addit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>otrC</td>\n",
       "      <td>Additionally, the significantly enhanced vanco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FosB</td>\n",
       "      <td>Thus, the P1 space group appears to be the res...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CpxR</td>\n",
       "      <td>In Klebsiella pneumoniae, CpxR is involved in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mfd</td>\n",
       "      <td>3) . Thus, the reduced spontaneous mutation ra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target_genes                                         statements  labels\n",
       "0        NDM-1  5% in Indian and Pakistan hospitals . In addit...       1\n",
       "1         otrC  Additionally, the significantly enhanced vanco...       1\n",
       "2         FosB  Thus, the P1 space group appears to be the res...       1\n",
       "3         CpxR  In Klebsiella pneumoniae, CpxR is involved in ...       1\n",
       "4          mfd  3) . Thus, the reduced spontaneous mutation ra...       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ad7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_statements, train_labels, train_genes=components(train_df)\n",
    "test_statements, test_labels, test_genes=components(test_df)\n",
    "val_statements, val_labels, val_genes=components(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf306eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "Device name: Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4169cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae6cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    \n",
    "set_seed(42)    # Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d53d765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'antibiotic', 'resistant']\n"
     ]
    }
   ],
   "source": [
    "prompt_template_encoding=tokenizer.tokenize(\" is antibiotic resistant \")\n",
    "print(prompt_template_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cff1873a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  4 18:22:34 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     On   | 00000000:3D:00.0 Off |                  Off |\n",
      "| 33%   32C    P8    35W / 260W |      3MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     On   | 00000000:3F:00.0 Off |                  Off |\n",
      "| 33%   32C    P8    23W / 260W |   1920MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro RTX 8000     On   | 00000000:40:00.0 Off |                  Off |\n",
      "| 33%   34C    P8    23W / 260W |   4784MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 8000     On   | 00000000:41:00.0 Off |                  Off |\n",
      "| 33%   31C    P8    32W / 260W |      3MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A     45260      C   ...onda3/envs/sid/bin/python     1917MiB |\n",
      "|    2   N/A  N/A     46556      C   ...onda3/envs/sid/bin/python     4781MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c88e7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare mask keywords\n",
    "\n",
    "bad_chars = [';', ':', \"(\", \")\", \"-\", \"'\", \"\\\"\", \"_\", \".\", \",\", \" \"]\n",
    "\n",
    "rule1=['resistan', 'efflux']\n",
    "\n",
    "rule2=[\"beta-lactamase\", \"beta lactamase\",  \"streptogramin inactivation enzyme\", \n",
    "\"fosfomycin inactivation enzyme\",\n",
    "\"tetracycline inactivation enzyme \", #AMR gene\n",
    "\"macrolide inactivation enzyme\",\n",
    "\"rifampin inactivation enzyme\",\n",
    "\"aminoglycoside acetyltransferase\", #\"AAC\",\n",
    "\"chloramphenicol phosphotransferase\",\n",
    "\"aminoglycoside phosphotransferase\", #\"APH\",\n",
    "\"chloramphenicol acetyltransferase\", #\"CAT\", \n",
    "\"aminoglycoside nucleotidyltransferase\", #\"ANT\",\n",
    "\"lincosamide nucleotidyltransferase\", #\"LNU\" \n",
    "\"streptothricin acetyltransferase\", #\"SAT\", \n",
    "\"fusidic acid inactivation enzyme\",\n",
    "\"Edeine acetyltransferase\", \n",
    "\"viomycin phosphotransferase\", \n",
    "\"ciprofloxacin phosphotransferase\",\n",
    "\"Bah amidohydrolase\",\n",
    "\"MDR\",\n",
    "\"MRSA\"]\n",
    "\n",
    "rule3=['bla', 'mec']\n",
    "\n",
    "rule4=['MIC', 'increase', 'fold']\n",
    "\n",
    "antibiotics_df=pd.read_excel('../../data/antibiotics.xlsx', sheet_name=0)\n",
    "temp_keywords=list(antibiotics_df['antibiotics'])\n",
    "temp_keywords = [y for x in [temp_keywords, rule1] for y in x]\n",
    "temp_keywords = [y for x in [temp_keywords, rule2] for y in x]\n",
    "temp_keywords = [y for x in [temp_keywords, rule3] for y in x]\n",
    "temp_keywords = [y for x in [temp_keywords, rule4] for y in x]\n",
    "mask_keywords=[]\n",
    "for i in range(len(temp_keywords)):\n",
    "    keywords=temp_keywords[i].replace(\"-\", \" \").split()\n",
    "#     for j in range(len(keywords)):\n",
    "#         filtered_word=''.join(k for k in keywords[j] if not k in bad_chars)\n",
    "    mask_keywords = [y for x in [mask_keywords, keywords] for y in x]\n",
    "mask_keywords=[i for i in mask_keywords if len(i)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c846c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_keywords=random.sample(mask_keywords, int(len(mask_keywords)*0.50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67372bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert_dev(statements, labels, genes):\n",
    "    pretokenized_encodings=[]\n",
    "    mask_indices=[]\n",
    "    prompt_encodings=[]\n",
    "    all_masks=[]\n",
    "    for idx in range(len(statements)):\n",
    "        sub_tokens=tokenizer.tokenize(statements[idx], add_special_tokens=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        gene_encoding=tokenizer.tokenize(genes[idx])\n",
    "        prompt=[]\n",
    "        prompt = [y for x in [prompt, gene_encoding] for y in x]\n",
    "        prompt = [y for x in [prompt, prompt_template_encoding] for y in x]\n",
    "        if(labels[idx]==1):\n",
    "            prompt.append('true')\n",
    "        else:\n",
    "            prompt.append('false')\n",
    "        if(len(sub_tokens)+len(prompt)+1>512):\n",
    "            sub_tokens=sub_tokens[:(512-len(prompt)-2)]\n",
    "            sub_tokens.append('[SEP]')\n",
    "        sub_tokens = [y for x in [sub_tokens, prompt] for y in x]\n",
    "        mask_index=len(sub_tokens)-1\n",
    "        mask_indices.append(mask_index)\n",
    "        prompt_encodings.append(prompt)\n",
    "        \n",
    "    return prompt_encodings, mask_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19863eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #prompt, label_mask, rule_mask\n",
    "# train_pretokenized_encodings, train_mask_indices=preprocessing_for_bert_dev(train_statements, train_labels, train_genes)\n",
    "# test_pretokenized_encodings, test_mask_indices, test_rule_masks=preprocessing_for_bert(test_statements, test_labels, test_genes)\n",
    "\n",
    "# train_pretokenized_encodings, mask_indices=preprocessing_for_bert(statements, labels, genes)\n",
    "val_pretokenized_encodings, val_mask_indices=preprocessing_for_bert_dev(val_statements, val_labels, val_genes)\n",
    "test_pretokenized_encodings, test_mask_indices=preprocessing_for_bert_dev(test_statements, test_labels, test_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5478cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt, label_mask, rule_mask\n",
    "# train_pretokenized_strings=[]\n",
    "# for idx in range(len(train_pretokenized_encodings)):\n",
    "#     train_pretokenized_strings.append(tokenizer.convert_tokens_to_string(train_pretokenized_encodings[idx]))\n",
    "\n",
    "val_pretokenized_strings=[]\n",
    "for idx in range(len(val_pretokenized_encodings)):\n",
    "    val_pretokenized_strings.append(tokenizer.convert_tokens_to_string(val_pretokenized_encodings[idx]))\n",
    "\n",
    "test_pretokenized_strings=[]\n",
    "for idx in range(len(test_pretokenized_encodings)):\n",
    "    test_pretokenized_strings.append(tokenizer.convert_tokens_to_string(test_pretokenized_encodings[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e9bb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_normal(statements, prompts, mask_ind):\n",
    "    inputs = tokenizer(\n",
    "        statements,prompts,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs['labels']=torch.ones(len(inputs['input_ids']),512, dtype=int)    \n",
    "    inputs['mask_idx']=torch.zeros(len(inputs['input_ids']),1, dtype=int)\n",
    "    for idx in range(len(inputs['input_ids'])):\n",
    "        inputs['labels'][idx][mask_ind[idx]]=inputs['input_ids'][idx][mask_ind[idx]]\n",
    "        inputs['input_ids'][idx][mask_ind[idx]]=tokenizer.mask_token_id\n",
    "        #################\n",
    "        ## MLM ##\n",
    "#         rule_mask=rule_masks[idx]\n",
    "#         for i in range(len(rule_mask)):\n",
    "#             if rule_mask[i]==1:\n",
    "#                 inputs['labels'][idx][i]=inputs['input_ids'][idx][i]\n",
    "#                 inputs['input_ids'][idx][i]=tokenizer.mask_token_id\n",
    "        #################\n",
    "        inputs['mask_idx'][idx][0]=mask_ind[idx]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "084cead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.encodings['input_ids'][index]\n",
    "        labels = self.encodings['labels'][index]\n",
    "        attention_mask = self.encodings['attention_mask'][index]\n",
    "        token_type_ids = self.encodings['token_type_ids'][index]\n",
    "        mask_idx = self.encodings['mask_idx'][index]\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'mask_idx': mask_idx\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "569f4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_inputs_normal=tokenize_normal(train_statements, train_pretokenized_strings, train_mask_indices)\n",
    "val_inputs_normal=tokenize_normal(val_statements, val_pretokenized_strings, val_mask_indices)\n",
    "test_inputs_normal=tokenize_normal(test_statements, test_pretokenized_strings, test_mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca4bddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# ftr = open('final_train_inputs_new_prompt2_MLM_50.txt', 'wb')\n",
    "# pickle.dump(train_inputs, ftr)\n",
    "# ftr.close()\n",
    "# # fv = open('val_inputs_prompt1_MLM.txt', 'wb')\n",
    "# # pickle.dump(val_inputs, fv)\n",
    "# # fv.close()\n",
    "# # ft = open('test_inputs_prompt1_MLM.txt', 'wb')\n",
    "# # pickle.dump(test_inputs, ft)\n",
    "# # ft.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07767a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('final_train_inputs_new_prompt2_MLM_50.txt', 'rb') as handle:\n",
    "#     data = handle.read()\n",
    "# train_inputs_MLM = pickle.loads(data)\n",
    "\n",
    "# # with open('val_inputs_prompt1_MLM.txt', 'rb') as handle:\n",
    "# #     data = handle.read()\n",
    "# # val_inputs_MLM = pickle.loads(data)\n",
    "\n",
    "# # with open('test_inputs_prompt1_MLM.txt', 'rb') as handle:\n",
    "# #     data = handle.read()\n",
    "# # test_inputs_MLM = pickle.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa110d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################\n",
    "# train_dataset=BookDataset(train_inputs)\n",
    "# ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01d32651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_normal=BookDataset(train_inputs_normal)\n",
    "val_dataset=BookDataset(val_inputs_normal)\n",
    "test_dataset=BookDataset(test_inputs_normal)\n",
    "# train_dataset=BookDataset(train_inputs_MLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9215d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8551378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, train_dataloader):\n",
    "\n",
    "#     set_seed(42)\n",
    "#     optimizer = AdamW(model.parameters(), lr=2e-05)\n",
    "#     model.train()\n",
    "\n",
    "#     # Tracking variables\n",
    "#     train_loss = []\n",
    "#     tp=0\n",
    "#     tn=0\n",
    "#     fp=0\n",
    "#     fn=0\n",
    "#     unk_pos=0\n",
    "#     unk_neg=0\n",
    "#     # for epoch in range(epochs):\n",
    "#     #     loop = tqdm(train_dataloader)\n",
    "#     for batch in train_dataloader:\n",
    "#         optimizer.zero_grad()\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs.loss   \n",
    "#         train_loss.append(loss.item())\n",
    "#         b_probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "#         # Get the predictions\n",
    "#         preds = torch.argmax(b_probabilities, dim=2)\n",
    "#     #     print(\"preds: \", preds[0])\n",
    "#     #     print(\"labels: \", labels[0])\n",
    "#         # Calculate the accuracy rate\n",
    "#         for i in range(len(batch['input_ids'])):\n",
    "#             pred=preds[i][batch['mask_idx'][i][0]]\n",
    "#             label=labels[i][batch['mask_idx'][i][0]]\n",
    "# #             print(\"pred: \", tokenizer.convert_ids_to_tokens(pred.item()))\n",
    "# #             print(\"label: \", tokenizer.convert_ids_to_tokens(label.item()))\n",
    "#             if pred == 6228 and label == 6228: \n",
    "#                 tp+=1\n",
    "#             elif pred == 6228 and label == 7198:\n",
    "#                 fp+=1\n",
    "#             elif pred == 7198 and label == 7198:\n",
    "#                 tn+=1\n",
    "#             elif pred == 7198 and label == 6228:\n",
    "#                 fn+=1\n",
    "#             elif label == 6228:\n",
    "#                 unk_pos+=1\n",
    "#             else:\n",
    "#                 unk_neg+=1\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "\n",
    "# #     train_loss = np.mean(train_loss)\n",
    "# #     #balanced accuracy = (tpr+tnr)/2\n",
    "# #     train_precision=tp/(tp+fp)\n",
    "# #     train_recall=tp/(tp+fn+unk_pos)\n",
    "# #     train_f1=2*train_precision*train_recall/(train_precision+train_recall)\n",
    "# #     train_balanced_accuracy = (train_recall+(tn/(tn+fp+unk_neg)))/2\n",
    "# #     train_accuracy = (tp+tn)/(tp+tn+fp+fn+unk_pos+unk_neg)\n",
    "#     train_total_not_predicted = unk_pos+unk_neg\n",
    "    \n",
    "    \n",
    "#     train_loss = np.mean(train_loss)\n",
    "# #     train_accuracy = np.mean(train_accuracy)\n",
    "    \n",
    "#     beta=0.5\n",
    "#     train_precision_pos=tp/(tp+fp)\n",
    "#     train_recall_pos=tp/(tp+fn+unk_pos)\n",
    "#     train_f1_pos=2*train_precision_pos*train_recall_pos/(train_precision_pos+train_recall_pos)\n",
    "# #     train_balanced_accuracy = (train_recall+(tn/(tn+fp)))/2\n",
    "#     train_precision_neg=tn/(tn+fn)\n",
    "#     train_recall_neg=tn/(tn+fp+unk_neg)\n",
    "#     train_f1_neg=2*train_precision_neg*train_recall_neg/(train_precision_neg+train_recall_neg)\n",
    "#     train_f1_beta_pos=((1+beta*beta)*train_precision_pos*train_recall_pos)/((1+beta*beta)*train_precision_pos*train_recall_pos+beta*beta*train_precision_pos+train_recall_pos)\n",
    "    \n",
    "#     train_accuracy = (tp+tn)/(tp+tn+fp+fn+unk_pos+unk_neg)\n",
    "# #     train_precision_neg=tn/(tn+fn)\n",
    "# #     train_recall_neg=tn/(tn+fp)\n",
    "# #     train_f1_neg=2*train_precision_pos*train_recall_pos/(train_precision_pos+train_recall_pos)\n",
    "#     train_f1_beta_neg=((1+beta*beta)*train_precision_neg*train_recall_neg)/((1+beta*beta)*train_precision_neg*train_recall_neg+beta*beta*train_precision_neg+train_recall_neg)\n",
    "    \n",
    "#     train_f1_beta=(train_f1_beta_pos+train_f1_beta_neg)/2\n",
    "#     print(f\" check_train_accuracy: {(tp+tn)/(tp+tn+fp+fn)}\")\n",
    "\n",
    "#     return [train_loss, train_accuracy, train_precision_pos, train_recall_pos, train_f1_pos, train_f1_beta_pos,train_precision_neg, train_recall_neg, train_f1_neg, train_f1_beta_neg, train_f1_beta, train_total_not_predicted]\n",
    "\n",
    "# #     return [train_loss, train_precision, train_recall, train_f1, train_balanced_accuracy, train_accuracy, train_total_not_predicted] \n",
    "\n",
    "#     #note total=11264\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "740a0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify loss function\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# model.to(device)\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "#     val_accuracy = []\n",
    "    val_loss = []\n",
    "    pred_label=[]\n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    unk_pos=0\n",
    "    unk_neg=0\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        b_attn_mask = batch['attention_mask'].to(device)\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_attn_mask, labels=b_labels)\n",
    "\n",
    "        # Compute loss\n",
    "#         loss=torch.nn.functional.cross_entropy(outputs.logits.view(-1, tokenizer.vocab_size), b_labels.view(-1))\n",
    "        loss=outputs.loss\n",
    "#         loss = loss_fn(outputs.logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "        b_probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "#         pred = torch.argmax( scores[0][7]).item()\n",
    "#         print(\"predicted token:\", pred, tokenizer.convert_ids_to_tokens([pred])  )\n",
    "#         print(NLLLos( logSoftmax(torch.unsqueeze(scores[0][7], 0)), torch.tensor([pred]))) #the same as F.cross_entropy(scores.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
    "        \n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(outputs.logits, dim=2)\n",
    "#         print(\"preds: \", preds[0])\n",
    "#         print(\"labels: \", b_labels[0])\n",
    "        # Calculate the accuracy rate\n",
    "#         for i in batch:\n",
    "        for i in range(len(batch['input_ids'])):\n",
    "            pred=preds[i][batch['mask_idx'][i][0]]\n",
    "            label=b_labels[i][batch['mask_idx'][i][0]]\n",
    "#             print(\"pred: \", preds[i])\n",
    "#             print(\"label: \", labels[i])\n",
    "            prob.append(b_probabilities[i][batch['mask_idx'][i][0]][6228].item())\n",
    "            if pred == 6228 and label == 6228: \n",
    "                tp+=1\n",
    "                pred_label.append(1)\n",
    "#                 prob_tp.append(b_probabilities[i][batch['mask_idx'][i][0]][6228].item())\n",
    "            elif pred == 6228 and label == 7198:\n",
    "                fp+=1\n",
    "                pred_label.append(1)\n",
    "#                 prob_fp.append(b_probabilities[i][batch['mask_idx'][i][0]][6228].item())\n",
    "            elif pred == 7198 and label == 7198:\n",
    "                tn+=1\n",
    "                pred_label.append(0)\n",
    "            elif pred == 7198 and label == 6228:\n",
    "                fn+=1    \n",
    "                pred_label.append(0)\n",
    "            elif label == 6228:\n",
    "                unk_pos+=1\n",
    "                pred_label.append(1)\n",
    "            else:\n",
    "                unk_neg+=1\n",
    "                pred_label.append(0)\n",
    "#         accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "#         val_accuracy.append(accuracy)\n",
    "    val_total_not_predicted = unk_pos+unk_neg\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    beta=0.5\n",
    "    val_loss = np.mean(val_loss)\n",
    "#     val_accuracy = np.mean(val_accuracy)\n",
    "    try:\n",
    "        val_precision_pos=tp/(tp+fp)\n",
    "    except ZeroDivisionError:\n",
    "        val_precision_pos=0\n",
    "    try:\n",
    "        val_recall_pos=tp/(tp+fn)\n",
    "    except ZeroDivisionError:\n",
    "        val_recall_pos=0\n",
    "    try:\n",
    "        val_f1_pos=2*val_precision_pos*val_recall_pos/(val_precision_pos+val_recall_pos)\n",
    "    except ZeroDivisionError:\n",
    "        val_f1_pos=0\n",
    "    try:\n",
    "        val_f1_beta_pos=((1+beta*beta)*val_precision_pos*val_recall_pos)/((1+beta*beta)*val_precision_pos*val_recall_pos+beta*beta*val_precision_pos+val_recall_pos)\n",
    "    except ZeroDivisionError:\n",
    "        val_f1_beta_pos=0\n",
    "#     try:\n",
    "#         val_balanced_accuracy = (val_recall+(tn/(tn+fp)))/2\n",
    "#     except ZeroDivisionError:\n",
    "#         val_balanced_accuracy =0\n",
    "    try:\n",
    "        val_precision_neg=tn/(tn+fn)\n",
    "    except ZeroDivisionError:\n",
    "        val_precision_neg=0\n",
    "    try:\n",
    "        val_recall_neg=tn/(tn+fp)\n",
    "    except ZeroDivisionError:\n",
    "        val_recall_neg=0\n",
    "    try:\n",
    "        val_f1_neg=2*val_precision_neg*val_recall_neg/(val_precision_neg+val_recall_neg)\n",
    "    except ZeroDivisionError:\n",
    "        val_f1_neg=0\n",
    "    try:\n",
    "        val_f1_beta_neg=((1+beta*beta)*val_precision_neg*val_recall_neg)/((1+beta*beta)*val_precision_neg*val_recall_neg+beta*beta*val_precision_neg+val_recall_neg)\n",
    "    except ZeroDivisionError:\n",
    "        val_f1_beta_neg=0\n",
    "    try:\n",
    "        val_f1_beta=(val_f1_beta_pos+val_f1_beta_neg)/2\n",
    "    except ZeroDivisionError:\n",
    "        val_f1_beta=0\n",
    "        \n",
    "    try:\n",
    "        val_accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "    except ZeroDivisionError:\n",
    "        val_accuracy=0\n",
    "        print(\"ZeroDivisionError\")\n",
    "\n",
    "    return [val_loss, val_accuracy, val_precision_pos, val_recall_pos, val_f1_pos, val_f1_beta_pos,val_precision_neg, val_recall_neg, val_f1_neg, val_f1_beta_neg, val_f1_beta, val_total_not_predicted, pred_label] \n",
    "        \n",
    "    \n",
    "#     val_accuracy = np.mean(val_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe5adb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=[8, 12]\n",
    "# learning_rate=[2e-5, 3e-5, 5e-5]\n",
    "epochs=[1,2,3,4,5,6,7,8,9,10]\n",
    "result_train=[[[] for bs in batch_size] for e in epochs]\n",
    "result_val=[[[] for bs in batch_size] for e in epochs]\n",
    "#result_val[e][lr][bs]=[metrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f1981",
   "metadata": {},
   "source": [
    "# Rule_mask_00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fba28b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss, val_accuracy, val_precision_pos, val_recall_pos, val_f1_pos, val_f1_beta_pos, ...negs, f1_beta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.9380981470169009\n",
      "batch_size: 8 epoch: 1 result_train: [0.0362932507766043, 0.7361776925535315, 0.9349609765859516, 0.7465643975711089, 0.8302087960906263, 0.4709107769221465, 0.9413471502590673, 0.725790987535954, 0.8196336731931788, 0.4704980525399851, 0.47070441473106583, 2694] result_val: [0.004400055992934742, 0.506993006993007, 0.5035714285714286, 0.986013986013986, 0.6666666666666666, 0.3582317073170732, 0.6666666666666666, 0.027972027972027972, 0.053691275167785234, 0.10695187165775401, 0.2325917894874136, 0]\n",
      "best_model_val_acc: 0.506993006993007\n",
      "best_model_val_f1: 0.6666666666666666\n",
      "best_model_val_f1_beta: 0.2325917894874136\n",
      " check_train_accuracy: 0.9867114953570285\n",
      "batch_size: 8 epoch: 2 result_train: [0.00012286152706973444, 0.984819431128156, 0.9859379993608182, 0.9859379993608182, 0.9859379993608182, 0.4964596073382684, 0.987487969201155, 0.9837008628954937, 0.9855907780979828, 0.49665988963113566, 0.496559748484702, 24] result_val: [0.009462978329365355, 0.5034965034965035, 0.5017543859649123, 1.0, 0.6682242990654206, 0.35785785785785784, 1.0, 0.006993006993006993, 0.013888888888888888, 0.03289473684210527, 0.19537629734998155, 0]\n",
      "best_model_val_f1: 0.6682242990654206\n",
      " check_train_accuracy: 0.9956052736715941\n",
      "batch_size: 8 epoch: 3 result_train: [3.0254067937843233e-05, 0.9955257270693513, 0.9948938886229456, 0.9963247043783956, 0.9956087824351296, 0.49879201932769085, 0.9963188220230473, 0.9947267497603068, 0.9955221493683032, 0.4989979959919839, 0.4988950076598374, 1] result_val: [0.009553000069870402, 0.5034965034965035, 0.5017543859649123, 1.0, 0.6682242990654206, 0.35785785785785784, 1.0, 0.006993006993006993, 0.013888888888888888, 0.03289473684210527, 0.19537629734998155, 0]\n",
      " check_train_accuracy: 0.9988015340364333\n",
      "batch_size: 8 epoch: 4 result_train: [9.14431264131381e-06, 0.9988015340364333, 0.9988812529966438, 0.9987216363055289, 0.9988014382740712, 0.4997121657925035, 0.9987218405496086, 0.9988814317673378, 0.9988016297834944, 0.49968824441637755, 0.4997002051044405, 0] result_val: [0.016844194758160742, 0.513986013986014, 0.5070921985815603, 1.0, 0.6729411764705883, 0.3600201409869084, 1.0, 0.027972027972027972, 0.054421768707483, 0.11173184357541902, 0.23587599228116368, 0]\n",
      "best_model_val_acc: 0.513986013986014\n",
      "best_model_val_f1: 0.6729411764705883\n",
      "best_model_val_f1_beta: 0.23587599228116368\n",
      " check_train_accuracy: 0.9983217453847998\n",
      "batch_size: 8 epoch: 5 result_train: [1.7444752514211156e-05, 0.9980824544582934, 0.998241968994726, 0.9980824544582934, 0.9981622053535757, 0.49955211465864746, 0.9984015345268542, 0.9980824544582934, 0.998241968994726, 0.4995840798566675, 0.4995680972576575, 3] result_val: [0.01305239267243996, 0.513986013986014, 0.5071428571428571, 0.993006993006993, 0.6713947990543735, 0.35985808413583376, 0.8333333333333334, 0.03496503496503497, 0.06711409395973154, 0.13020833333333334, 0.24503320873458356, 0]\n",
      "best_model_val_f1_beta: 0.24503320873458356\n",
      " check_train_accuracy: 0.9994407158836689\n",
      "batch_size: 8 epoch: 6 result_train: [3.94975803667179e-06, 0.9994407158836689, 0.9993609202748043, 0.9995206136145733, 0.9994407605656308, 0.49984816762294426, 0.9995205369985616, 0.9993608181527645, 0.9994406711945665, 0.49987211458533154, 0.4998601411041379, 0] result_val: [0.019897884625144063, 0.513986013986014, 0.5071942446043165, 0.986013986013986, 0.6698337292161519, 0.3596938775510204, 0.75, 0.04195804195804196, 0.07947019867549669, 0.14634146341463414, 0.25301767048282725, 0]\n",
      "best_model_val_f1_beta: 0.25301767048282725\n",
      " check_train_accuracy: 0.9996005113454778\n",
      "batch_size: 8 epoch: 7 result_train: [2.054056615570195e-06, 0.9996005113454778, 0.9996803579990411, 0.9995206136145733, 0.9996004794246903, 0.499912085804255, 0.9995206902061032, 0.9996804090763822, 0.9996005432611648, 0.49988813245117775, 0.4999001091277164, 0] result_val: [0.016827504132405816, 0.513986013986014, 0.5073529411764706, 0.965034965034965, 0.6650602409638554, 0.3591879229567933, 0.6428571428571429, 0.06293706293706294, 0.11464968152866242, 0.18442622950819676, 0.271807076232495, 0]\n",
      "best_model_val_f1_beta: 0.271807076232495\n",
      " check_train_accuracy: 0.9996005113454778\n",
      "batch_size: 8 epoch: 8 result_train: [2.590152297932508e-06, 0.9996005113454778, 0.9996803579990411, 0.9995206136145733, 0.9996004794246903, 0.499912085804255, 0.9995206902061032, 0.9996804090763822, 0.9996005432611648, 0.49988813245117775, 0.4999001091277164, 0] result_val: [0.017459289322155964, 0.506993006993007, 0.5035714285714286, 0.986013986013986, 0.6666666666666666, 0.3582317073170732, 0.6666666666666666, 0.027972027972027972, 0.053691275167785234, 0.10695187165775401, 0.2325917894874136, 0]\n",
      " check_train_accuracy: 1.0\n",
      "batch_size: 8 epoch: 9 result_train: [2.3353708743209007e-08, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 0] result_val: [0.020625074116031913, 0.5034965034965035, 0.501779359430605, 0.986013986013986, 0.6650943396226415, 0.3575050709939148, 0.6, 0.02097902097902098, 0.04054054054054055, 0.08426966292134833, 0.22088736695763156, 0]\n",
      " check_train_accuracy: 1.0\n",
      "batch_size: 8 epoch: 10 result_train: [4.235881836802924e-09, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 0] result_val: [0.022799906582872825, 0.506993006993007, 0.5035460992907801, 0.993006993006993, 0.668235294117647, 0.35840484603735484, 0.75, 0.02097902097902098, 0.04081632653061225, 0.08620689655172414, 0.2223058712945395, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check_train_accuracy: 0.916621134302262\n",
      "batch_size: 12 epoch: 1 result_train: [0.052350147738435535, 0.6701821668264621, 0.9086101121218532, 0.6863215084691595, 0.7819754210286755, 0.4604318089234793, 0.9251808318264014, 0.6540428251837648, 0.7663358921550272, 0.4607262657871632, 0.46057903735532124, 3365] result_val: [0.00338371904520803, 0.5454545454545454, 0.5243445692883895, 0.9790209790209791, 0.6829268292682926, 0.3663003663003663, 0.8421052631578947, 0.11188811188811189, 0.19753086419753088, 0.26755852842809363, 0.31692944736422995, 0]\n",
      "best_model_val_acc: 0.5454545454545454\n",
      "best_model_val_f1: 0.6829268292682926\n",
      "best_model_val_f1_beta: 0.31692944736422995\n",
      " check_train_accuracy: 0.9847683181016514\n",
      "batch_size: 12 epoch: 2 result_train: [0.00018597814739200724, 0.9814637264301694, 0.9824505424377792, 0.9840204538191115, 0.9832348714673479, 0.49565357372826785, 0.9871092491137609, 0.9789069990412272, 0.9829910141206675, 0.496337827326938, 0.4959957005276029, 42] result_val: [0.005309210838049694, 0.527972027972028, 0.5150375939849624, 0.958041958041958, 0.6699266503667481, 0.36205073995771675, 0.7, 0.0979020979020979, 0.17177914110429449, 0.2389078498293515, 0.30047929489353414, 0]\n",
      " check_train_accuracy: 0.9938454160338902\n",
      "batch_size: 12 epoch: 3 result_train: [4.7468990179496576e-05, 0.9934483860658357, 0.9931375678263645, 0.9944071588366891, 0.9937719578409454, 0.49834232906770026, 0.9945556445156125, 0.9924896132949824, 0.9935215548268416, 0.49853113511951586, 0.49843673209360806, 5] result_val: [0.006133526012386407, 0.5699300699300699, 0.5390625, 0.965034965034965, 0.6917293233082706, 0.37156704361873993, 0.8333333333333334, 0.17482517482517482, 0.2890173410404624, 0.32216494845360827, 0.3468659960361741, 0]\n",
      "best_model_val_acc: 0.5699300699300699\n",
      "best_model_val_f1: 0.6917293233082706\n",
      "best_model_val_f1_beta: 0.3468659960361741\n",
      " check_train_accuracy: 0.9972035794183445\n",
      "batch_size: 12 epoch: 4 result_train: [1.9832358974419132e-05, 0.9972035794183445, 0.9974420463629097, 0.9969638862256311, 0.9972029089746663, 0.49933571302802815, 0.9969653409998402, 0.9974432726110578, 0.9972042495406982, 0.4992641412848733, 0.4992999271564507, 0] result_val: [0.009808073227121072, 0.5244755244755245, 0.5128205128205128, 0.9790209790209791, 0.673076923076923, 0.36175710594315247, 0.7692307692307693, 0.06993006993006994, 0.12820512820512822, 0.20408163265306123, 0.28291936929810685, 0]\n",
      " check_train_accuracy: 0.9989610804763046\n",
      "batch_size: 12 epoch: 5 result_train: [1.2027128594496188e-05, 0.9987216363055289, 0.9987220447284345, 0.9990412272291467, 0.9988816104809074, 0.49969628185044274, 0.9992003838157685, 0.9984020453819111, 0.998801055071537, 0.4997600383938569, 0.4997281601221498, 3] result_val: [0.015107661879696291, 0.506993006993007, 0.5035211267605634, 1.0, 0.6697892271662763, 0.3585757271815447, 1.0, 0.013986013986013986, 0.027586206896551724, 0.06211180124223602, 0.21034376421189035, 0]\n",
      " check_train_accuracy: 1.0\n",
      "batch_size: 12 epoch: 6 result_train: [1.2703806150012757e-07, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 0] result_val: [0.02062747431422178, 0.506993006993007, 0.5035211267605634, 1.0, 0.6697892271662763, 0.3585757271815447, 1.0, 0.013986013986013986, 0.027586206896551724, 0.06211180124223602, 0.21034376421189035, 0]\n",
      " check_train_accuracy: 1.0\n",
      "batch_size: 12 epoch: 7 result_train: [1.2041292361206221e-08, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 0] result_val: [0.021997685517362395, 0.506993006993007, 0.5035211267605634, 1.0, 0.6697892271662763, 0.3585757271815447, 1.0, 0.013986013986013986, 0.027586206896551724, 0.06211180124223602, 0.21034376421189035, 0]\n",
      " check_train_accuracy: 1.0\n",
      "batch_size: 12 epoch: 8 result_train: [4.530739854699803e-09, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 0] result_val: [0.021222862277581012, 0.506993006993007, 0.5035211267605634, 1.0, 0.6697892271662763, 0.3585757271815447, 1.0, 0.013986013986013986, 0.027586206896551724, 0.06211180124223602, 0.21034376421189035, 0]\n",
      " check_train_accuracy: 1.0\n",
      "batch_size: 12 epoch: 9 result_train: [3.0261310352859956e-09, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 0] result_val: [0.021564801541709627, 0.506993006993007, 0.5035211267605634, 1.0, 0.6697892271662763, 0.3585757271815447, 1.0, 0.013986013986013986, 0.027586206896551724, 0.06211180124223602, 0.21034376421189035, 0]\n",
      " check_train_accuracy: 1.0\n",
      "batch_size: 12 epoch: 10 result_train: [2.2643978511731575e-09, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 0] result_val: [0.021818568315187758, 0.506993006993007, 0.5035211267605634, 1.0, 0.6697892271662763, 0.3585757271815447, 1.0, 0.013986013986013986, 0.027586206896551724, 0.06211180124223602, 0.21034376421189035, 0]\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "# print(\"val_loss, val_accuracy, val_precision_pos, val_recall_pos, val_f1_pos, val_f1_beta_pos, ...negs, f1_beta\")\n",
    "# best_model_val_acc=-1\n",
    "# best_model_val_f1=-1\n",
    "# best_model_val_f1_beta=-1\n",
    "# for bs in range(len(batch_size)):\n",
    "#     train_dataloader = torch.utils.data.DataLoader(\n",
    "#         train_dataset_normal,\n",
    "#         batch_size=batch_size[bs],\n",
    "#         #shuffle=True\n",
    "#     )\n",
    "#     val_dataloader = torch.utils.data.DataLoader(\n",
    "#         val_dataset,\n",
    "#         batch_size=batch_size[bs],\n",
    "#     )\n",
    "# #     train_dataloader_MLM = torch.utils.data.DataLoader(\n",
    "# #                     train_dataset,\n",
    "# #                     batch_size=batch_size[bs],\n",
    "# #                     #shuffle=True\n",
    "# #     )\n",
    "#     torch.cuda.empty_cache() \n",
    "#     model = BertForMaskedLM.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "#     model.to(device)\n",
    "#     for e in range(len(epochs)): \n",
    "        \n",
    "#         with open(\"prompt1_00_result_train.txt\", \"wb\") as ft:   #Pickling\n",
    "#             result_train[e][bs]=train(model, train_dataloader)\n",
    "#             pickle.dump(result_train, ft)\n",
    "#         ft.close()\n",
    "#         with open(\"prompt1_00_result_val.txt\", \"wb\") as fv:   #Pickling\n",
    "#             result_val[e][bs]=evaluate(model, val_dataloader)\n",
    "#             pickle.dump(result_val, fv)\n",
    "#         fv.close()\n",
    "\n",
    "#         print(f\"batch_size: {batch_size[bs]} epoch: {epochs[e]} result_train: {result_train[e][bs]} result_val: {result_val[e][bs]}\")\n",
    "# #             if(e>1 and result_val[e][lr][bs][5]<result_val[e-1][lr][bs][5]):\n",
    "# #                 print(\"Early stopping\")\n",
    "# #                 break\n",
    "#         if result_val[e][bs][1] > best_model_val_acc:\n",
    "#             best_model_val_acc=result_val[e][bs][1]\n",
    "# #                 !rmdir model_prompt1_MLM\n",
    "#             print(f\"best_model_val_acc: {best_model_val_acc}\")\n",
    "#             model.save_pretrained(save_directory='./prompt1/model/microf1/prompt1_00/saved_model')\n",
    "        \n",
    "#         if result_val[e][bs][4] > best_model_val_f1:\n",
    "#             best_model_val_f1=result_val[e][bs][4]\n",
    "# #                 !rmdir model_prompt1_MLM\n",
    "#             print(f\"best_model_val_f1: {best_model_val_f1}\")\n",
    "#             model.save_pretrained(save_directory='./prompt1/model/f1/prompt1_00/saved_model')\n",
    "#         if result_val[e][bs][10] > best_model_val_f1_beta:\n",
    "#             best_model_val_f1_beta=result_val[e][bs][10]\n",
    "# #                 !rmdir model_prompt1_MLM\n",
    "#             print(f\"best_model_val_f1_beta: {best_model_val_f1_beta}\")\n",
    "#             model.save_pretrained(save_directory='./prompt1/model/f1beta/prompt1_00/saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ec0b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d735f",
   "metadata": {},
   "source": [
    "adjustment for the runtime error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dec191df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5699300699300699, 3, 12)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pick the best model\n",
    "max_result=-1\n",
    "for e in range(len(epochs)):\n",
    "    for bs in range(len(batch_size)):\n",
    "        if(len(result_val[e][bs])!=0):\n",
    "            if(result_val[e][bs][1]>max_result):\n",
    "                max_result=result_val[e][bs][1]\n",
    "                max_e=epochs[e]\n",
    "                max_bs=batch_size[bs]\n",
    "                    \n",
    "max_result,max_e,max_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e672a7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00338371904520803,\n",
       " 0.5454545454545454,\n",
       " 0.5243445692883895,\n",
       " 0.9790209790209791,\n",
       " 0.6829268292682926,\n",
       " 0.3663003663003663,\n",
       " 0.8421052631578947,\n",
       " 0.11188811188811189,\n",
       " 0.19753086419753088,\n",
       " 0.26755852842809363,\n",
       " 0.31692944736422995,\n",
       " 0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_val[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e361c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=12,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a81203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=12,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff1e5b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 12 epoch: 3 result_test: [0.0056376808249216075, 0.5936132983377078, 0.5719825496849249, 0.9624796084828712, 0.7175433262389784, 0.38366497593965404, 0.7937219730941704, 0.1669811320754717, 0.27591582229150424, 0.31194924215720826, 0.34780710904843115, 0]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "model = BertForMaskedLM.from_pretrained(\"../../prompt1/model/microf1/prompt1_00/saved_model\")\n",
    "model.to(device)\n",
    "max_result_test=evaluate(model, test_dataloader)\n",
    "print(f\"batch_size: {max_bs} epoch: {max_e} result_test: {max_result_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "448084cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 12 epoch: 3 result_test: [0.006133526012386407, 0.5699300699300699, 0.5390625, 0.965034965034965, 0.6917293233082706, 0.37156704361873993, 0.8333333333333334, 0.17482517482517482, 0.2890173410404624, 0.32216494845360827, 0.3468659960361741, 0, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "model = BertForMaskedLM.from_pretrained(\"../../prompt1/prompt1/model/microf1/prompt1_00/saved_model\")\n",
    "model.to(device)\n",
    "prob_tp=[]\n",
    "prob_fp=[]\n",
    "prob=[]\n",
    "max_result_test=evaluate(model, val_dataloader)\n",
    "print(f\"batch_size: {12} epoch: {3} result_test: {max_result_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f738ceb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "118\n",
      "0.5390625\n",
      "0.965034965034965\n",
      "0.6917293233082706\n"
     ]
    }
   ],
   "source": [
    "print(len(prob_tp))\n",
    "print(len(prob_fp))\n",
    "tp=len(prob_tp)\n",
    "fp=len(prob_fp)\n",
    "precision=tp/(tp+fp)\n",
    "recall=tp/143\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "802e597f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.999987006187439,\n",
       " 0.9983225464820862,\n",
       " 0.999991774559021,\n",
       " 0.9982933402061462,\n",
       " 0.9998859167098999,\n",
       " 0.999930739402771,\n",
       " 0.9994188547134399,\n",
       " 0.9997108578681946,\n",
       " 0.9999858140945435,\n",
       " 0.9999589920043945,\n",
       " 0.9987497329711914,\n",
       " 0.9864532351493835,\n",
       " 0.9999954700469971,\n",
       " 0.9916287660598755,\n",
       " 0.9999717473983765,\n",
       " 0.9999109506607056,\n",
       " 0.9991077780723572,\n",
       " 0.99982088804245,\n",
       " 0.9981914162635803,\n",
       " 0.9999964237213135,\n",
       " 0.8956517577171326,\n",
       " 0.9916883111000061,\n",
       " 0.9999438524246216,\n",
       " 0.999950647354126,\n",
       " 0.9953450560569763,\n",
       " 0.9984654188156128,\n",
       " 0.9803815484046936,\n",
       " 0.9783726334571838,\n",
       " 0.9938368201255798,\n",
       " 0.9996391534805298,\n",
       " 0.9987803101539612,\n",
       " 0.9999175071716309,\n",
       " 0.9120179414749146,\n",
       " 0.9998760223388672,\n",
       " 0.9999970197677612,\n",
       " 0.9999099969863892,\n",
       " 0.9850530028343201,\n",
       " 0.8488094210624695,\n",
       " 0.9924578666687012,\n",
       " 0.9999914169311523,\n",
       " 0.9997753500938416,\n",
       " 0.9931100606918335,\n",
       " 0.9979475140571594,\n",
       " 0.9979378581047058,\n",
       " 0.9996160268783569,\n",
       " 0.9979070425033569,\n",
       " 0.9991681575775146,\n",
       " 0.9993554949760437,\n",
       " 0.9999775886535645,\n",
       " 0.9999911785125732,\n",
       " 0.9999676942825317,\n",
       " 0.9999587535858154,\n",
       " 0.9999862909317017,\n",
       " 0.7170774936676025,\n",
       " 0.9999889135360718,\n",
       " 0.9998375177383423,\n",
       " 0.9999877214431763,\n",
       " 0.9968505501747131,\n",
       " 0.9986427426338196,\n",
       " 0.9994543194770813,\n",
       " 0.9962592124938965,\n",
       " 0.9995056390762329,\n",
       " 0.9998782873153687,\n",
       " 0.9932094216346741,\n",
       " 0.9997751116752625,\n",
       " 0.9992885589599609,\n",
       " 0.9985170960426331,\n",
       " 0.907156229019165,\n",
       " 0.9996910095214844,\n",
       " 0.9999871253967285,\n",
       " 0.9979456067085266,\n",
       " 0.9998413324356079,\n",
       " 0.9999432563781738,\n",
       " 0.9999847412109375,\n",
       " 0.9999500513076782,\n",
       " 0.9999507665634155,\n",
       " 0.999984622001648,\n",
       " 0.9995962977409363,\n",
       " 0.9999797344207764,\n",
       " 0.9995394945144653,\n",
       " 0.9999938011169434,\n",
       " 0.9867132902145386,\n",
       " 0.999426007270813,\n",
       " 0.954471230506897,\n",
       " 0.8808521032333374,\n",
       " 0.9944671392440796,\n",
       " 0.9997883439064026,\n",
       " 0.999640703201294,\n",
       " 0.9991602897644043,\n",
       " 0.9891397356987,\n",
       " 0.8943173289299011,\n",
       " 0.9999958276748657,\n",
       " 0.9942732453346252,\n",
       " 0.9978912472724915,\n",
       " 0.9650844931602478,\n",
       " 0.9999229907989502,\n",
       " 0.9998905658721924,\n",
       " 0.9997647404670715,\n",
       " 0.9995198249816895,\n",
       " 0.9999531507492065,\n",
       " 0.9999796152114868,\n",
       " 0.6071286797523499,\n",
       " 0.9983388185501099,\n",
       " 0.9998953342437744,\n",
       " 0.9999736547470093,\n",
       " 0.9968607425689697,\n",
       " 0.9999266862869263,\n",
       " 0.9999762773513794,\n",
       " 0.9993624091148376,\n",
       " 0.999722421169281,\n",
       " 0.9990476965904236,\n",
       " 0.9953746199607849,\n",
       " 0.9999516010284424,\n",
       " 0.999976396560669,\n",
       " 0.9998830556869507,\n",
       " 0.9999217987060547,\n",
       " 0.9867454171180725,\n",
       " 0.9999785423278809,\n",
       " 0.9945268034934998,\n",
       " 0.9999566078186035,\n",
       " 0.9999775886535645,\n",
       " 0.9999902248382568,\n",
       " 0.9999394416809082,\n",
       " 0.9985688924789429,\n",
       " 0.9999922513961792,\n",
       " 0.9999157190322876,\n",
       " 0.9999923706054688,\n",
       " 0.9999861717224121,\n",
       " 0.9993252754211426,\n",
       " 0.9977140426635742,\n",
       " 0.9929138422012329,\n",
       " 0.9999262094497681,\n",
       " 0.9997028708457947,\n",
       " 0.9999758005142212,\n",
       " 0.9925004839897156,\n",
       " 0.9988813996315002,\n",
       " 0.999871015548706,\n",
       " 0.9999488592147827]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b79eaff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "12\n",
      "0.7\n",
      "0.1958041958041958\n",
      "0.30601092896174864\n"
     ]
    }
   ],
   "source": [
    "tp=0\n",
    "fp=0\n",
    "for i in range(len(prob_tp)):\n",
    "    if prob_tp[i]>0.9999758 :\n",
    "        tp+=1\n",
    "for i in range(len(prob_fp)):\n",
    "    if prob_fp[i]>0.9999758 :\n",
    "        fp+=1\n",
    "print(tp)\n",
    "print(fp)\n",
    "precision=tp/(tp+fp)\n",
    "recall=tp/143\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb1f1601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999370574951172,\n",
       " 0.9997527003288269,\n",
       " 0.9999934434890747,\n",
       " 0.9999828338623047,\n",
       " 0.9875956773757935,\n",
       " 0.9994445443153381,\n",
       " 0.999606192111969,\n",
       " 0.9971023201942444,\n",
       " 0.999680757522583,\n",
       " 0.6370437741279602,\n",
       " 0.9997876286506653,\n",
       " 0.9998310804367065,\n",
       " 0.9995880722999573,\n",
       " 0.9999701976776123,\n",
       " 0.9974743723869324,\n",
       " 0.9999610185623169,\n",
       " 0.9985907673835754,\n",
       " 0.9998738765716553,\n",
       " 0.9995265007019043,\n",
       " 0.9999594688415527,\n",
       " 0.9988705515861511,\n",
       " 0.9993197917938232,\n",
       " 0.9999912977218628,\n",
       " 0.9998747110366821,\n",
       " 0.9982275366783142,\n",
       " 0.9998612403869629,\n",
       " 0.9987413287162781,\n",
       " 0.9999805688858032,\n",
       " 0.9998881816864014,\n",
       " 0.9997420907020569,\n",
       " 0.9999909400939941,\n",
       " 0.9971489310264587,\n",
       " 0.9930790066719055,\n",
       " 0.9987659454345703,\n",
       " 0.9672781825065613,\n",
       " 0.9878236055374146,\n",
       " 0.999886155128479,\n",
       " 0.9999197721481323,\n",
       " 0.998554527759552,\n",
       " 0.9999817609786987,\n",
       " 0.997901201248169,\n",
       " 0.9973033666610718,\n",
       " 0.9265697002410889,\n",
       " 0.9991607666015625,\n",
       " 0.997066080570221,\n",
       " 0.9999938011169434,\n",
       " 0.9988343119621277,\n",
       " 0.9981493949890137,\n",
       " 0.9864132404327393,\n",
       " 0.999920129776001,\n",
       " 0.7021856904029846,\n",
       " 0.867488443851471,\n",
       " 0.7183729410171509,\n",
       " 0.9996176958084106,\n",
       " 0.9154406189918518,\n",
       " 0.9920699000358582,\n",
       " 0.9999653100967407,\n",
       " 0.9968202114105225,\n",
       " 0.9999579191207886,\n",
       " 0.6500316858291626,\n",
       " 0.9984285235404968,\n",
       " 0.9989742040634155,\n",
       " 0.9997627139091492,\n",
       " 0.984762966632843,\n",
       " 0.9999607801437378,\n",
       " 0.9999709129333496,\n",
       " 0.9962387084960938,\n",
       " 0.9862896203994751,\n",
       " 0.9985907673835754,\n",
       " 0.9999469518661499,\n",
       " 0.9999867677688599,\n",
       " 0.9999734163284302,\n",
       " 0.9996127486228943,\n",
       " 0.9999912977218628,\n",
       " 0.9999978542327881,\n",
       " 0.9940043091773987,\n",
       " 0.9872275590896606,\n",
       " 0.9996194839477539,\n",
       " 0.9092212319374084,\n",
       " 0.9998489618301392,\n",
       " 0.907939076423645,\n",
       " 0.9953052997589111,\n",
       " 0.9994258880615234,\n",
       " 0.9999480247497559,\n",
       " 0.9966366291046143,\n",
       " 0.9997561573982239,\n",
       " 0.9965403079986572,\n",
       " 0.9972190856933594,\n",
       " 0.9634407162666321,\n",
       " 0.992235004901886,\n",
       " 0.9990699887275696,\n",
       " 0.9978353381156921,\n",
       " 0.9874376654624939,\n",
       " 0.9973152279853821,\n",
       " 0.9970036149024963,\n",
       " 0.9967129230499268,\n",
       " 0.9999314546585083,\n",
       " 0.9999464750289917,\n",
       " 0.9981821775436401,\n",
       " 0.992938756942749,\n",
       " 0.8577955961227417,\n",
       " 0.9951836466789246,\n",
       " 0.9999774694442749,\n",
       " 0.9997337460517883,\n",
       " 0.9872874021530151,\n",
       " 0.9965497255325317,\n",
       " 0.9976322650909424,\n",
       " 0.9997560381889343,\n",
       " 0.9996111989021301,\n",
       " 0.9996811151504517,\n",
       " 0.998456597328186,\n",
       " 0.9907118678092957,\n",
       " 0.9994031190872192,\n",
       " 0.7315015196800232,\n",
       " 0.9999938011169434,\n",
       " 0.998426079750061,\n",
       " 0.999937891960144,\n",
       " 0.9998942613601685]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecda1b3",
   "metadata": {},
   "source": [
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0843613b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 12 epoch: 3 result_test: [0.0008179426875929482, 0.9171312584573749, 1.0, 0.9171312584573749, 0.9567746124961178, 0.4955226324932298, 0.0, 0, 0, 0, 0.2477613162466149, 0]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "model = BertForMaskedLM.from_pretrained(\"../../prompt1/prompt1/model/microf1/prompt1_00/saved_model\")\n",
    "model.to(device)\n",
    "prob_tp=[]\n",
    "prob_fp=[]\n",
    "prob=[]\n",
    "max_result_noisy_train=evaluate(model, noisy_train_dataloader)\n",
    "print(f\"batch_size: {12} epoch: {3} result_test: {max_result_noisy_train[:-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9850857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"val_labels/prompt1_00_labels.txt\", \"wb\") as f:   #Pickling\n",
    "    prompt1_labels=max_result_test[-1]\n",
    "    pickle.dump(prompt1_labels, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b24ac1a",
   "metadata": {},
   "source": [
    "# 2000eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8297d282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 12 epoch: 3 result_test: [0.0027303161439998216, 0.5990990990990991, 0.5768386388583974, 0.9722479185938946, 0.7240785394419567, 0.38571638285378745, 0.8295454545454546, 0.15921483097055616, 0.26715462031107046, 0.3105061675882604, 0.3481112752210239, 2, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "model = BertForMaskedLM.from_pretrained(\"../../prompt1/prompt1/model/f1/prompt1_25/saved_model\")\n",
    "model.to(device)\n",
    "prob=[]\n",
    "max_result_test=evaluate(model, test_dataloader)\n",
    "print(f\"batch_size: {12} epoch: {3} result_test: {max_result_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a3f5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"2000/test_labels/prompt1_25_labels.txt\", \"wb\") as f:   #Pickling\n",
    "    prompt1_labels=max_result_test[-1]\n",
    "    pickle.dump(prompt1_labels, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a92343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"2000/test_labels/prompt1_25_prob.txt\", \"wb\") as f:   #Pickling\n",
    "    pickle.dump(prob, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "028dd704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 12 epoch: 3 result_test: [0.00338371904520803, 0.5454545454545454, 0.5243445692883895, 0.9790209790209791, 0.6829268292682926, 0.3663003663003663, 0.8421052631578947, 0.11188811188811189, 0.19753086419753088, 0.26755852842809363, 0.31692944736422995, 0, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "model = BertForMaskedLM.from_pretrained(\"../../prompt1/prompt1/model/f1/prompt1_25/saved_model\")\n",
    "model.to(device)\n",
    "prob=[]\n",
    "max_result_test=evaluate(model, val_dataloader)\n",
    "print(f\"batch_size: {12} epoch: {3} result_test: {max_result_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37ab8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"2000/val_labels/prompt1_25_labels.txt\", \"wb\") as f:   #Pickling\n",
    "    prompt1_labels=max_result_test[-1]\n",
    "    pickle.dump(prompt1_labels, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c46a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"2000/val_labels/prompt1_25_prob.txt\", \"wb\") as f:   #Pickling\n",
    "    pickle.dump(prob, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18fc1fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 12 epoch: 3 result_test: [0.005576186052717085, 0.597, 0.5766463752075263, 0.9621421975992613, 0.7211072664359862, 0.38532652910287696, 0.7875647668393783, 0.16575790621592149, 0.2738738738738739, 0.31033074724377296, 0.347828638173325, 0, [1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "model = BertForMaskedLM.from_pretrained(\"../../prompt1/prompt1/model/f1/prompt1_00/saved_model\")\n",
    "model.to(device)\n",
    "prob=[]\n",
    "max_result_test=evaluate(model, test_dataloader)\n",
    "print(f\"batch_size: {12} epoch: {3} result_test: {max_result_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59882b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"2000/test_labels/prompt1_00_labels.txt\", \"wb\") as f:   #Pickling\n",
    "    prompt1_labels=max_result_test[-1]\n",
    "    pickle.dump(prompt1_labels, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5ebeae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"2000/test_labels/prompt1_00_prob.txt\", \"wb\") as f:   #Pickling\n",
    "    pickle.dump(prob, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2658056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 12 epoch: 3 result_test: [0.006133526012386407, 0.5699300699300699, 0.5390625, 0.965034965034965, 0.6917293233082706, 0.37156704361873993, 0.8333333333333334, 0.17482517482517482, 0.2890173410404624, 0.32216494845360827, 0.3468659960361741, 0, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "model = BertForMaskedLM.from_pretrained(\"../../prompt1/prompt1/model/f1/prompt1_00/saved_model\")\n",
    "model.to(device)\n",
    "prob=[]\n",
    "max_result_test=evaluate(model, val_dataloader)\n",
    "print(f\"batch_size: {12} epoch: {3} result_test: {max_result_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae5b7e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"2000/val_labels/prompt1_00_labels.txt\", \"wb\") as f:   #Pickling\n",
    "    prompt1_labels=max_result_test[-1]\n",
    "    pickle.dump(prompt1_labels, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bc09538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"2000/val_labels/prompt1_00_prob.txt\", \"wb\") as f:   #Pickling\n",
    "    pickle.dump(prob, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02bc30c",
   "metadata": {},
   "source": [
    "Connor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "105622e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 12 epoch: 3 result_test: [0.00391549773566011, 0.43, 0.37362637362637363, 1.0, 0.544, 0.2992957746478873, 1.0, 0.13636363636363635, 0.24000000000000002, 0.30612244897959184, 0.3027091118137396, 0, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "model = BertForMaskedLM.from_pretrained(\"../../prompt1/prompt1/model/f1/prompt1_25/saved_model\")\n",
    "model.to(device)\n",
    "prob=[]\n",
    "max_result_test=evaluate(model, test_dataloader)\n",
    "print(f\"batch_size: {12} epoch: {3} result_test: {max_result_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf90c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
